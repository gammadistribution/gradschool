\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amsthm, amssymb, amsfonts, breqn}


\theoremstyle{definition}
\newtheorem{problem}{Problem}
\renewcommand*{\proofname}{Solution}
\newenvironment{custompbm}[1]
  {\renewcommand\theproblem{#1}\problem}
  {\endproblem}
\renewcommand{\theenumi}{\alph{enumi}}


\newcommand{\E}{\text{E}}
\newcommand{\V}{\text{Var}}
\newcommand{\Co}[2]{\text{Cov}({#1}, {#2})}
\newcommand{\pdf}{\text{pdf}}
\newcommand{\pmf}{\text{pmf}}
\newcommand{\me}{\mathrm{e}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand{\vect}[1]{\boldsymbol{#1}}
\newcommand{\mx}[1][t]{\mu_X({#1})}
\newcommand{\gx}[2]{\gamma_X({#1}, {#2})}


\title{Homework Assignment 4}
\author{Matthew Tiger}


\begin{document}


\maketitle


% Problem 2.3
\begin{custompbm}{2.3}
  Find the ACVF of the time series $X_t = Z_t + aZ_{t-1} + bZ_{t-2}$ where
  $Z_t \sim WN(0, \sigma^2)$ when:
  \begin{enumerate}
    \item $a=0.3$, $b=-0.4$, and $\sigma^2=1$.
    \item $a=-1.2$, $b=-1.6$, and $\sigma^2=0.25$.
  \end{enumerate}
\end{custompbm}

\begin{proof}
  The ACVF of the time series $\{X_t\}$, $\gamma_X(h)$, is by definition:
  \begin{align}\label{acvf}
    \gamma_X(h)
    &= \Co{X_{t+h}}{X_t} \notag \\
    &= \Co{Z_{t+h} + aZ_{t+h-1} + bZ_{t+h-2}}{Z_{t} + aZ_{t-1} + bZ_{t-2}} \notag \\
    &= \Co{Z_{t+h}}{Z_t} + a\Co{Z_{t+h}}{Z_{t-1}} + b\Co{Z_{t+h}}{Z_{t-2}} \notag \\
    & \hspace{5mm} + a\Co{Z_{t+h-1}}{Z_t} + a^2\Co{Z_{t+h-1}}{Z_{t-1}} + ab\Co{Z_{t+h-1}}{Z_{t-2}} \notag \\
    & \hspace{5mm} + b\Co{Z_{t+h-2}}{Z_t} + ab\Co{Z_{t+h-2}}{Z_{t-1}} + b^2\Co{Z_{t+h-2}}{Z_{t-2}}.
  \end{align}
  Using \eqref{acvf}, we can see that since $Z_t \sim WN(0, \sigma^2)$,
  \begin{align*}
    \gamma_X(h) =
    \begin{cases}
      (1 + a^2 + b^2)\sigma^2 & \text{if $h=0$}\\
      a(1 + b)\sigma^2 & \text{if $h=\pm1$}\\
      b\sigma^2 & \text{if $h=\pm2$}\\
      0 & \text{otherwise}
    \end{cases}.
  \end{align*}
  Therefore, when
  \begin{enumerate}
    \item $a=0.3$, $b=-0.4$, and $\sigma^2=1$, the ACVF of $\{X_t\}$ is:
      \[
        \begin{cases}
          1.25 & \text{if $h=0$}\\
          0.18 & \text{if $h=\pm1$}\\
          -0.4 & \text{if $h=\pm2$}\\
          0 & \text{otherwise}
        \end{cases}
      \]
    \item $a=-1.2$, $b=-1.6$, and $\sigma^2=0.25$, the ACVF of $\{X_t\}$ is:
      \[
        \begin{cases}
          1.25 & \text{if $h=0$}\\
          0.18 & \text{if $h=\pm1$}\\
          -0.4 & \text{if $h=\pm2$}\\
          0 & \text{otherwise}
        \end{cases}
      \]
  \end{enumerate}
\end{proof}


% Problem 2.5
\begin{custompbm}{2.5}
  Suppose that $\{X_t, t=0, \pm1, \dots\}$ is stationary and that $|\theta|<1$.
  Show that for each fixed $n$ the sequence
  \[
    S_m = \sum_{j=1}^{m} \theta ^ j X_{n-j}
  \]
  is convergent absolutely and in mean square as $m \to \infty$.
\end{custompbm}

\begin{proof}
  Let $a_j = \theta ^ j X_{n-j}$. Since each $X_i$ is a random variable, each $X_i$
  maps to a real, non-infinite value so let $X = \max\{|X_i|\}$. Then to see that $S_m$ is convergent absolutely
  as $m \to \infty$, notice that
  \begin{align*}
    \sum_{j=1}^{m} |a_j|
    &= \sum_{j=1}^{m} |\theta ^ j X_{n-j}| \\
    &= \sum_{j=1}^{m} |\theta| ^ j |X_{n-j}|\\
    &\leq \sum_{j=1}^{m} X|\theta| ^ j = \sum_{j=1}^{m} b_j = T_m
  \end{align*}
  Since $|\theta|<1$, we know that as $m \to \infty$, the partial sum $\sum_{j=1}^m X|\theta|^j \to 0$
  and it must hold that $T_m \to 0$. Thus, we know that as $m \to \infty$, $\sum_{j=1}^{m} |a_j|$ converges to some $L$ since $|a_j| \leq b_j$
  and $T_m$ is convergent. Therefore, $S_m$ is convergent absolutely.

  To see that $S_m$ is convergent in the mean square, it suffices to show that
  $\E(S_m - S_l)^2 \to 0$ as $m, l \to \infty$.

  Without loss of generality, assume that $m > l > 0$. Notice that
  $S_m - S_l = \sum_{j=1}^m a_j - \sum_{j=1}^n a_j = \sum_{j=l+1}^{m} a_j$.
  Thus, $$\E(S_m - S_l) = \E(\sum_{j=l+1}^{m} a_j) = \sum_{j=l+1}^m \E(a_j).$$
  It is clear that $\E(a_j) = \E(\theta^j X_{n-j}) = \theta^j \E(X_{n-j})$. Since
  $\{X_t\}$ is a stationary time series, its expectation does not depend on $t$,
  so say $\E(X_{n-j}) = \mu_X$. Then
  \begin{align*}
    \E(S_m - S_l)
    &= \sum_{j=l+1}^m \theta^j\E(X_{n-j}) \\
    &=\mu_X \sum_{j=l+1}^m \theta^j \\
    &= \frac{\mu_X\theta^{l+1}(1-\theta^{m-l-1})}{1-\theta}
  \end{align*}
  Since $|\theta| < 1$, it is clear then that $\E(S_m - S_l)^2 \to 0$ as $m,l \to \infty$
  showing that $S_m$ is convergent in mean square for any $n$.
\end{proof}


% Problem 2.11
\begin{custompbm}{2.11}
  Suppose that in a sample of size 100 from an AR(1) process with mean $\mu$,
  $\phi=0.6$, and $\sigma^2 = 2$ we obtain $\bar{x_{100}} = 0.271$.
  Construct an approximate 95\% confidence interval for $\mu$. Are the data
  compatible with the hypothesis that $\mu = 0$.
\end{custompbm}

\begin{proof}
  Note that since AR(1) is a linear model, $\bar{X_n}$ is approximately normal
  with mean $\mu$ for large $n$ and an approximate 95\% confidence interval for
  $\mu$ is
  \[
    \left(\bar{X_n} - \frac{1.96\nu^{1/2}}{\sqrt{n}}, \bar{X_n} + \frac{1.96\nu^{1/2}}{\sqrt{n}}\right)
  \]
  where $\nu = \sum_{|h|<\infty}\gamma_X(h)$.

  Since $\{X_t\}$ is an AR(1) process, we know $\gamma_X(h) = \gamma_X(0)\phi^{|h|}$
  where $\gamma_X(0) = \sigma^2 / (1 - \phi^2)$. Thus
  \begin{align*}
    \nu = \sum_{|h| < \infty} \gamma_X(h)
    &= \sum_{|h| < \infty} \frac{\sigma^2 \phi^{|h|}}{1 - \phi^2} \\
    &= \frac{\sigma ^ 2}{1 - \phi^2} \left(1 + 2\sum_{h=1}^\infty\phi^h\right) \\
    &= \frac{\sigma ^ 2}{1 - \phi^2} \left(1 + \frac{2\phi}{1-\phi}\right) \\
    &= \frac{\sigma ^ 2(1+\phi)}{(1-\phi)(1 - \phi^2)} = \frac{\sigma ^ 2}{(1-\phi)^2}
  \end{align*}
  If $\phi = 0.6$ and $\sigma^2 = 2$, then $\nu = 2 / (1 - 0.6)^2 = 12.5$. Since $n=100$,
  $\bar{x_n} = \bar{x_{100}} = 0.271$, and an approximate 95\% confidence interval
  for $\mu$ is
  \[
    \left(0.271 - \frac{1.96(12.5)^{1/2}}{\sqrt{100}}, 0.271 + \frac{1.96(12.5)^{1/2}}{\sqrt{100}}\right)
  \]
  or $(-0.42197, 0.96397)$. Given this confidence interval, it is plausible that
  $\mu = 0.$
\end{proof}


\end{document}

\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amsthm, amssymb, amsfonts, breqn}


\theoremstyle{definition}
\newtheorem{problem}{Problem}
\renewcommand*{\proofname}{Solution}
\newenvironment{custompbm}[1]
  {\renewcommand\theproblem{#1}\problem}
  {\endproblem}
\renewcommand{\theenumi}{\alph{enumi}}


\newcommand{\E}{\text{E}}
\newcommand{\V}{\text{Var}}
\newcommand{\Co}[2]{\text{Cov}({#1}, {#2})}
\newcommand{\pdf}{\text{pdf}}
\newcommand{\pmf}{\text{pmf}}
\newcommand{\me}{\mathrm{e}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand{\vect}[1]{\boldsymbol{#1}}
\newcommand{\mx}[1][t]{\mu_X({#1})}
\newcommand{\gx}[2]{\gamma_X({#1}, {#2})}


\title{Homework Assignment 4}
\author{Matthew Tiger}


\begin{document}


\maketitle


% Problem 2.3
\begin{custompbm}{2.3}
  Find the ACVF of the time series $X_t = Z_t + aZ_{t-1} + bZ_{t-2}$ where
  $Z_t \sim WN(0, \sigma^2)$ when:
  \begin{enumerate}
    \item $a=0.3$, $b=-0.4$, and $\sigma^2=1$.
    \item $a=-1.2$, $b=-1.6$, and $\sigma^2=0.25$.
  \end{enumerate}
\end{custompbm}

\begin{proof}
  The ACVF of the time series $\{X_t\}$, $\gamma_X(h)$, is by definition:
  \begin{align}\label{acvf}
    \gamma_X(h)
    &= \Co{X_{t+h}}{X_t} \notag \\
    &= \Co{Z_{t+h} + aZ_{t+h-1} + bZ_{t+h-2}}{Z_{t} + aZ_{t-1} + bZ_{t-2}} \notag \\
    &= \Co{Z_{t+h}}{Z_t} + a\Co{Z_{t+h}}{Z_{t-1}} + b\Co{Z_{t+h}}{Z_{t-2}} \notag \\
    & \hspace{5mm} + a\Co{Z_{t+h-1}}{Z_t} + a^2\Co{Z_{t+h-1}}{Z_{t-1}} + ab\Co{Z_{t+h-1}}{Z_{t-2}} \notag \\
    & \hspace{5mm} + b\Co{Z_{t+h-2}}{Z_t} + ab\Co{Z_{t+h-2}}{Z_{t-1}} + b^2\Co{Z_{t+h-2}}{Z_{t-2}}.
  \end{align}
  Using \eqref{acvf}, we can see that since $Z_t \sim WN(0, \sigma^2)$,
  \begin{align*}
    \gamma_X(h) =
    \begin{cases}
      (1 + a^2 + b^2)\sigma^2 & \text{if $h=0$}\\
      a(1 + b)\sigma^2 & \text{if $h=\pm1$}\\
      b\sigma^2 & \text{if $h=\pm2$}\\
      0 & \text{otherwise}
    \end{cases}.
  \end{align*}
  Therefore, when
  \begin{enumerate}
    \item $a=0.3$, $b=-0.4$, and $\sigma^2=1$, the ACVF of $\{X_t\}$ is:
      \[
        \begin{cases}
          1.25 & \text{if $h=0$}\\
          0.18 & \text{if $h=\pm1$}\\
          -0.4 & \text{if $h=\pm2$}\\
          0 & \text{otherwise}
        \end{cases}
      \]
    \item $a=-1.2$, $b=-1.6$, and $\sigma^2=0.25$, the ACVF of $\{X_t\}$ is:
      \[
        \begin{cases}
          1.25 & \text{if $h=0$}\\
          0.18 & \text{if $h=\pm1$}\\
          -0.4 & \text{if $h=\pm2$}\\
          0 & \text{otherwise}
        \end{cases}
      \]
  \end{enumerate}
\end{proof}


% Problem 2.5
\begin{custompbm}{2.5}
  Suppose that $\{X_t, t=0, \pm1, \dots\}$ is stationary and that $|\theta|<1$.
  Show that for each fixed $n$ the sequence
  \[
    S_m = \sum_{j=1}^{m} \theta ^ j X_{n-j}
  \]
  is convergent absolutely and in mean square as $m \to \infty$.
\end{custompbm}

\begin{proof}
  Let $a_j = \theta ^ j X_{n-j}$. Since each $X_i$ is a random variable, each $X_i$
  maps to a real, non-infinite value so let $X = \max\{|X_i|\}$. Then to see that $S_m$ is convergent absolutely
  as $m \to \infty$, notice that
  \begin{align*}
    \sum_{j=1}^{m} |a_j|
    &= \sum_{j=1}^{m} |\theta ^ j X_{n-j}| \\
    &= \sum_{j=1}^{m} |\theta| ^ j |X_{n-j}|\\
    &\leq \sum_{j=1}^{m} X|\theta| ^ j = \sum_{j=1}^{m} b_j = T_m
  \end{align*}
  Since $|\theta|<1$, we know that as $m \to \infty$, the partial sum $\sum_{j=1}^m X|\theta|^j \to 0$
  and it must hold that $T_m \to 0$. Thus, we know that as $m \to \infty$, $\sum_{j=1}^{m} |a_j|$ converges to some $L$ since $|a_j| \leq b_j$
  and $T_m$ is convergent. Therefore, $S_m$ is convergent absolutely.

  To see that $S_m$ is convergent in the mean square, it suffices to show that
  $\E(S_m - S_l)^2 \to 0$ as $m, l \to \infty$.

  Without loss of generality, assume that $m > l > 0$. Notice that
  $S_m - S_l = \sum_{j=1}^m a_j - \sum_{j=1}^n a_j = \sum_{j=l+1}^{m} a_j$.
  Thus, $$\E(S_m - S_l) = \E\left(\sum_{j=l+1}^{m} a_j\right) = \sum_{j=l+1}^m \E(a_j).$$
  It is clear that $\E(a_j) = \E(\theta^j X_{n-j}) = \theta^j \E(X_{n-j})$. Since
  $\{X_t\}$ is a stationary time series, its expectation does not depend on $t$,
  so say $\E(X_{n-j}) = \mu_X$. Then
  \begin{align*}
    \E(S_m - S_l)
    &= \sum_{j=l+1}^m \theta^j\E(X_{n-j}) \\
    &=\mu_X \sum_{j=l+1}^m \theta^j \\
    &= \frac{\mu_X\theta^{l+1}(1-\theta^{m-l-1})}{1-\theta}
  \end{align*}
  Since $|\theta| < 1$, it is clear then that $\E(S_m - S_l)^2 \to 0$ as $m,l \to \infty$
  showing that $S_m$ is convergent in mean square for any $n$.
\end{proof}


% Problem 2.11
\begin{custompbm}{2.11}
  Suppose that in a sample of size 100 from an AR(1) process with mean $\mu$,
  $\phi=0.6$, and $\sigma^2 = 2$ we obtain $\bar{x}_{100} = 0.271$.
  Construct an approximate 95\% confidence interval for $\mu$. Are the data
  compatible with the hypothesis that $\mu = 0$.
\end{custompbm}

\begin{proof}
  Note that since AR(1) is a linear model, $\bar{X}_n$ is approximately normal
  with mean $\mu$ for large $n$ and an approximate 95\% confidence interval for
  $\mu$ is
  \[
    \left(\bar{X}_{n} - \frac{1.96\nu^{1/2}}{\sqrt{n}}, \bar{X}_{n} + \frac{1.96\nu^{1/2}}{\sqrt{n}}\right)
  \]
  where $\nu = \sum_{|h|<\infty}\gamma_X(h)$.

  Since $\{X_t\}$ is an AR(1) process, we know that $\gamma_X(h) = \gamma_X(0)\phi^{|h|}$
  where $\gamma_X(0) = \sigma^2 / (1 - \phi^2)$. Thus
  \begin{align*}
    \nu = \sum_{|h| < \infty} \gamma_X(h)
    &= \sum_{|h| < \infty} \frac{\sigma^2 \phi^{|h|}}{1 - \phi^2} \\
    &= \frac{\sigma ^ 2}{1 - \phi^2} \left(1 + 2\sum_{h=1}^\infty\phi^h\right) \\
    &= \frac{\sigma ^ 2}{1 - \phi^2} \left(1 + \frac{2\phi}{1-\phi}\right) \\
    &= \frac{\sigma ^ 2(1+\phi)}{(1-\phi)(1 - \phi^2)} = \frac{\sigma ^ 2}{(1-\phi)^2}
  \end{align*}
  If $\phi = 0.6$ and $\sigma^2 = 2$, then $\nu = 2 / (1 - 0.6)^2 = 12.5$. Since $n=100$,
  $\bar{x}_{n} = \bar{x}_{100} = 0.271$, and an approximate 95\% confidence interval
  for $\mu$ is
  \[
    \left(0.271 - \frac{1.96(12.5)^{1/2}}{\sqrt{100}}, 0.271 + \frac{1.96(12.5)^{1/2}}{\sqrt{100}}\right)
  \]
  or $(-0.42197, 0.96397)$. Given this confidence interval, it is plausible that
  $\mu = 0.$
\end{proof}


% Problem 2.12
\begin{custompbm}{2.12}
  Suppose that in a sample of size 100 from an MA(1) process with mean $\mu$,
  $\theta=-0.6$, and $\sigma^2 = 1$ we obtain $\bar{x}_{100} = 0.157$.
  Construct an approximate 95\% confidence interval for $\mu$. Are the data
  compatible with the hypothesis that $\mu = 0$.
\end{custompbm}

\begin{proof}
  Note that since MA(1) is a linear model, $\bar{X}_{n}$ is approximately normal
  with mean $\mu$ for large $n$ and an approximate 95\% confidence interval for
  $\mu$ is
  \[
    \left(\bar{X}_{n} - \frac{1.96\nu^{1/2}}{\sqrt{n}}, \bar{X}_{n} + \frac{1.96\nu^{1/2}}{\sqrt{n}}\right)
  \]
  where $\nu = \sum_{|h|<\infty}\gamma_X(h)$.

  Since $\{X_t\}$ is an MA(1) process, we know that
  \begin{align*}
    \gamma_X(h) =
    \begin{cases}
      \sigma^2(1+\theta^2) & \text{if $h=0$}\\
      \sigma^2\theta & \text{if $h=\pm 1$}\\
      0 & \text{otherwise}
    \end{cases}.
  \end{align*}
  Thus
  \begin{align*}
    \nu = \sum_{|h| < \infty} \gamma_X(h)
    &= \sigma^2(1+\theta^2) +2\sigma^2\theta = \sigma^2(1+\theta)^2
  \end{align*}
  If $\theta = -0.6$ and $\sigma^2 = 1$, then $\nu = (1 - 0.6))^2 = 0.16$. Since $n=100$,
  $\bar{x}_{n} = \bar{x}_{100} = 0.157$, and an approximate 95\% confidence interval
  for $\mu$ is
  \[
    \left(0.157 - \frac{1.96(0.16)^{1/2}}{\sqrt{100}}, 0.157 + \frac{1.96(0.16)^{1/2}}{\sqrt{100}}\right)
  \]
  or $(0.15198, 0.16202)$. Given this confidence interval, it is not plausible that
  $\mu = 0.$
\end{proof}


% Problem 2.13
\begin{custompbm}{2.13}
  Suppose that in a sample of size 100, we obtain $\hat{\rho}(1) = 0.438$
  and $\hat{\rho}(2) = 0.145$.
  \begin{enumerate}
    \item Assuming that the data were generated from an AR(1) model, construct
      approximate 95 \% confidence intervals for both $\rho(1)$ and $\rho(2)$.
      Based on these two confidence intervals, are the data consistent with an
      AR(1) model with $\phi = 0.8$
    \item Assuming that the data were generated from an MA(1) model, construct
      approximate 95 \% confidence intervals for both $\rho(1)$ and $\rho(2)$.
      Based on these two confidence intervals, are the data consistent with an
      MA(1) model with $\theta = 0.6$
  \end{enumerate}
\end{custompbm}

\begin{proof}
  Note that since AR(1) and MA(1) are linear models, we know that for large $n$,
  the vector $\vect{\hat{\rho}} = (\hat{\rho}(1), \dots, \hat{\rho}(k))^\intercal$
  is a multivariate normal random variable with mean $\vect{\rho} = (\rho(1), \dots, \rho(k))^\intercal$
  and covariance matrix $n^{-1}W$ where $w_{ij}$ is defined by Bartlett's formula.

  Hence, an approximate 95\% confidence interval for
  $\rho(i)$ is
  \[
    \left(\hat{\rho}(i) - 1.96\V(\hat{\rho}(i))^{1/2}, \hat{\rho}(i) + 1.96\V(\hat{\rho}(i))^{1/2}\right).
  \]
  Since $\hat{\rho}(i) \sim N(\rho(i), n^{-1}w_{ii})$, we know $\V(\hat{\rho}(i)) = n^{-1}w_{ii}$ and the
  approximate 95\% confidence interval for $\rho(i)$ is
  \begin{align}\label{ci}
    \left(\hat{\rho}(i) - \frac{1.96w_{ii}^{1/2}}{\sqrt{n}}, \hat{\rho}(i) - \frac{1.96w_{ii}^{1/2}}{\sqrt{n}}\right).
  \end{align}
  \begin{enumerate}
    \item If the data were generated from an AR(1) model,
      then $$w_{ii} = (1 - \phi^{2i})(1 + \phi^2)(1 - \phi^2)^{-1} - 2i\phi^{2i}$$
      If $\phi = 0.8$, then $w_{11} = 0.36$ and $w_{22} = 1.0512$. Using \eqref{ci}, we know
      that the 95\% confidence interval for $\rho(1)$ is (0.3204, 0.556) and
      the 95\% confidence interval for $\rho(2)$ is (-0.0560, 0.3640)
      If $\phi = 0.8$, then the actual values of $\rho(1) = 0.8$ and $\rho(2) = 0.64$
      suggest that the data are consistent with an AR(1) model with
      $\phi = 0.8.$
    \item If the data were generated from an MA(1) model,
      then $w_{11} = 1 - 3 \rho(1)^2 + 4 \rho(1)^4$ and $w_{22} = 1 + 2 \rho(1)^2$ where $\rho(1) = \theta / (1 + \theta^2)$.
      If $\theta = 0.6$, then $w_{11} = 0.5676$ and $w_{22} = 1.3893$. Using \eqref{ci}, we know
      that the 95\% confidence interval for $\rho(1)$ is (0.2903, 0.5857) and
      the 95\% confidence interval for $\rho(2)$ is (-0.0860, 0.3760)
      If $\theta = 0.6$, then the actual values of $\rho(1) = 0.4412$ and $\rho(2) = 0$
      suggest that the data are consistent with an MA(1) model with
      $\theta = 0.6.$
  \end{enumerate}
\end{proof}


\end{document}

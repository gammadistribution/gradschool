\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amsthm, amssymb, amsfonts, breqn, graphicx}


\theoremstyle{definition}
\newtheorem{problem}{Problem}
\renewcommand*{\proofname}{Solution}
\newenvironment{custompbm}[1]
  {\renewcommand\theproblem{#1}\problem}
  {\endproblem}
\renewcommand{\theenumi}{\alph{enumi}}


\newcommand{\E}{\text{E}}
\newcommand{\V}{\text{Var}}
\newcommand{\Co}[2]{\text{Cov}\left({#1}, {#2}\right)}
\newcommand{\pdf}{\text{pdf}}
\newcommand{\pmf}{\text{pmf}}
\newcommand{\me}{\mathrm{e}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand{\vect}[1]{\boldsymbol{#1}}
\newcommand{\mx}[1][t]{\mu_X({#1})}
\newcommand{\gx}[2]{\gamma_X({#1}, {#2})}


\title{Homework Assignment 9}
\author{Matthew Tiger}


\begin{document}


\maketitle


% Problem 2.20
\begin{custompbm}{2.20}
  In the innovations algorithm, show that for each $n\geq 2$, the innovation
  $X_n - \hat{X}_n$ is uncorrelated with $X_1, X_2, \dots, X_{n-1}$. Conclude that
  $X_n - \hat{X}_n$ is uncorrelated with the innovations
  $X_1 - \hat{X}_1, X_2 - \hat{X}_2, \dots, X_{n-1} - \hat{X}_{n-1}$
\end{custompbm}

\begin{proof}
  Note that if $n \geq 2$, then $\hat{X}_n = P_{n-1}X_n = a_0 + a_1 X_{n-1} + \dots + a_{n-1}X_{1}$
  where $a_1, \dots, a_{n-1}$ is the solution to the system of equations
  \[
    \begin{bmatrix}
      \gamma(0) & \gamma(1) & \hdots & \gamma(n-2) \\
      \gamma(1) & \gamma(0) & \hdots & \gamma(n-3) \\
      \vdots & \vdots & \ddots & \vdots \\
      \gamma(n-2) & \gamma(n-3) & \hdots & \gamma(0) \\
    \end{bmatrix}
    \begin{bmatrix}
      a_1 \\
      a_2 \\
      \vdots \\
      a_{n-1} \\
    \end{bmatrix}
    =
    \begin{bmatrix}
      \gamma(1) \\
      \gamma(2) \\
      \vdots \\
      \gamma(n-1) \\
    \end{bmatrix}.
  \]
  Now, for $0 < i < n$,
  \begin{align}\label{sum}
    \Co{X_n - \hat{X}_n}{X_i} &= \Co{X_n}{X_i} - \Co{\sum_{j=1}^{n-1} a_j X_{n-j}}{X_i} \notag\\
    &= \Co{X_n}{X_i} - \sum_{j=1}^{n-1} a_j \Co{X_{n-j}}{X_i} \notag \\
    &= \gamma(n-i) - \sum_{j=1}^{n-1} a_j \gamma(n-j-i)
  \end{align}
  From the above system of equations it is clear that $\sum_{j=1}^{n-1} a_j \gamma(n-j-i) = \gamma(n-i)$
  and that $\Co{X_n - \hat{X}_n}{X_i} = 0$. Therefore,
  $X_n - \hat{X}_n$ is uncorrelated with $X_i$ for $n\geq 2$ and $0 < i < n$.

  Now, say $\hat{X}_i = b_0 + b_1 X_{i-1} + \dots + b_{i-1}X_{1}$for $0 < i < n$. Then
  \begin{align*}
    \Co{X_n - \hat{X}_n}{X_i - \hat{X_i}} &= \Co{X_n - \hat{X}_n}{X_i} - \Co{X_n - \hat{X}_n}{\hat{X}_i} \\
    &= -\Co{X_n - \hat{X}_n}{\sum_{j=1}^{i-1} b_j X_{i-j}} \notag\\
    &= -\sum_{j=1}^{i-1} b_j \Co{X_n - \hat{X}_n}{X_{i-j}} = 0 \notag \\
  \end{align*}
  from our above results since $i - j < n$ for $0 < i < n$, $0 < j < i$, and $n \geq 2$.
  Therefore $X_n - \hat{X}_n$ is uncorrelated with $X_i - \hat{X}_i$ for $n\geq 2$ and $0 < i < n$.


\end{proof}


% Problem 2.21
\begin{custompbm}{2.21}
  Let $X_1, X_2, X_3, X_4, X_5$ be observations from the MA(1) model.
  \[
    X_t = Z_t + \theta Z_{t-1}, \{Z_t\} \sim \text{WN}(0, \sigma^2).
  \]
  \begin{enumerate}
    \item Find the best linear estimate of the missing value $X_3$ in terms of
      $X_1$ and $X_2$.
    \item Find the best linear estimate of the missing value $X_3$ in terms of
      $X_4$ and $X_5$.
    \item Find the best linear estimate of the missing value $X_3$ in terms of
      $X_1$, $X_2$, $X_4$, and $X_5$.
    \item Compute the mean squared errors for each of the estimates in (a),
      (b), and (c).
  \end{enumerate}
\end{custompbm}

\begin{proof}
  If $Y$ and $W_n, \dots, W_1$ are random variables, then for $\vect{W} = (W_n, \dots, W_1)^\intercal$
  and $\vect{\mu}_W= \left(\E(W_n), \dots, \E(W_1)\right)^\intercal$, the best linear
  predictor of $Y$ in terms of $\{1, W_n, \dots, W_1\}$ is
  \[
    P(Y|\vect{W}) = \E(Y) + \vect{a}^\intercal (\vect{W} - \vect{\mu_W})
  \]
  where $\vect{a}$ is the solution of $\Gamma \vect{a} = \gamma$ for
  $\Gamma = \Co{\vect{W}}{\vect{W}}$ and $\gamma = \Co{Y}{\vect{W}}$.

  Also, note for an MA(1) process, the autocovariance function is defined as
  \[
    \gamma_X(h) =
    \begin{cases}
      \sigma^2 (1 + \theta^2) & \text{if $h = 0$}\\
      \sigma^2 \theta & \text{if $|h| = 1$}\\
      0 & \text{if $|h| > 1$}\\
    \end{cases}
  \]
  \begin{enumerate}
    \item Using the above, set $Y = X_3$ and $W = (X_2, X_1)^\intercal$. Then
      \begin{align*}
        \Gamma = \Co{\vect{W}}{\vect{W}} =
        \begin{bmatrix} \gamma_X(0) & \gamma_X(1) \\ \gamma_X(1) & \gamma_X(0) \end{bmatrix} =
        \sigma^2 \begin{bmatrix} 1 + \theta^2 & \theta \\ \theta & 1 + \theta^2 \end{bmatrix}
      \end{align*}
      and
      \begin{align*}
        \gamma = \begin{bmatrix} \gamma_X(1) \\ \gamma_X(2) \end{bmatrix}
        = \sigma^2 \begin{bmatrix} \theta \\ 0 \end{bmatrix}.
      \end{align*}
      The solution to the system of equations $\Gamma \vect{a} = \gamma$ is
      $$\vect{a} = \frac{\theta}{1 + \theta^2 + \theta^4} \begin{bmatrix}1 + \theta^2 \\ -\theta\end{bmatrix}.$$
      Therefore, the best predictor of $X_3$ is
      \begin{align*}
        P(X_3|\vect{W}) &= \E(X_3) + \vect{a}^\intercal (\vect{W} - \vect{\mu}_W) \\
        &= \frac{\theta}{1 + \theta^2 + \theta^4} ((1 + \theta^2) X_2 - \theta X_1)
      \end{align*}

    \item Using the above, set $Y = X_3$ and $W = (X_5, X_4)^\intercal$. Then
      \begin{align*}
        \Gamma = \Co{\vect{W}}{\vect{W}} =
        \begin{bmatrix} \gamma_X(0) & \gamma_X(1) \\ \gamma_X(1) & \gamma_X(0) \end{bmatrix} =
        \sigma^2 \begin{bmatrix} 1 + \theta^2 & \theta \\ \theta & 1 + \theta^2 \end{bmatrix}
      \end{align*}
      and
      \begin{align*}
        \gamma = \begin{bmatrix} \gamma_X(2) \\ \gamma_X(1) \end{bmatrix}
        = \sigma^2 \begin{bmatrix} 0 \\ \theta \end{bmatrix}.
      \end{align*}
      The solution to the system of equations $\Gamma \vect{a} = \gamma$ is
      $$\vect{a} = \frac{\theta}{1 + \theta^2 + \theta^4} \begin{bmatrix}-\theta\\ 1 + \theta^2 \end{bmatrix}.$$
      Therefore, the best predictor of $X_3$ is
      \begin{align*}
        P(X_3|\vect{W}) &= \E(X_3) + \vect{a}^\intercal (\vect{W} - \vect{\mu}_W) \\
        &= \frac{\theta}{1 + \theta^2 + \theta^4} (- \theta  X_5 + (1 + \theta^2) X_4)
      \end{align*}

    \item Using the above, set $Y = X_3$ and $W = (X_5, X_4, X_2, X_1)^\intercal$. Then
      \begin{align*}
        \Gamma = \Co{\vect{W}}{\vect{W}} &=
        \phantom{\sigma^2}
        \begin{bmatrix}
          \gamma_X(0) & \gamma_X(1) & \gamma_X(3) & \gamma_X(4) \\
          \gamma_X(1) & \gamma_X(0) & \gamma_X(2) & \gamma_X(3) \\
          \gamma_X(3) & \gamma_X(2) & \gamma_X(0) & \gamma_X(1) \\
          \gamma_X(4) & \gamma_X(3) & \gamma_X(1) & \gamma_X(0) \\
        \end{bmatrix}\\
        &=
        \sigma^2
        \begin{bmatrix}
          1 + \theta^2 & \theta & 0 & 0 \\
          \theta & 1 + \theta^2 & 0 & 0 \\
          0 & 0 & 1 + \theta^2 & \theta \\
          0 & 0 & \theta & 1 + \theta^2 \\
        \end{bmatrix}
      \end{align*}
      and
      \begin{align*}
        \gamma =
        \begin{bmatrix}
          \gamma_X(2) \\
          \gamma_X(1) \\
          \gamma_X(1) \\
          \gamma_X(2) \\
        \end{bmatrix}
        = \sigma^2
        \begin{bmatrix}
          0 \\
          \theta \\
          \theta \\
          0 \\
        \end{bmatrix}.
      \end{align*}
      The solution to the system of equations $\Gamma \vect{a} = \gamma$ is
      \begin{align*}
        \vect{a} = \frac{\theta}{1 + \theta^2 + \theta^4}
        \begin{bmatrix}
          -\theta \\
          1 + \theta^2 \\
          1 + \theta^2 \\
          -\theta \\
        \end{bmatrix}.
      \end{align*}
      Therefore, the best predictor of $X_3$ is
      \begin{align*}
        P(X_3|\vect{W}) &= \E(X_3) + \vect{a}^\intercal (\vect{W} - \vect{\mu}_W) \\
        &= \frac{\theta}{1 + \theta^2 + \theta^4} (- \theta  X_5 + (1 + \theta^2) X_4 + (1 + \theta^2) X_2 -\theta X_1)
      \end{align*}

    \item The mean squared error of the predictor in terms of the known random
      variables is $\E\left[ (Y - P(Y|\vect{W}))^2 \right] = \V(Y) - \vect{a}^\intercal \gamma$.

      Therefore, the mean squared error for:

      (a) is $\E\left[ (X_3 - P(X_3|\vect{W}))^2 \right] =
      \sigma^2 (1 + \theta^2) - \frac{\sigma^2\theta^2(1 + \theta^2)}{1 + \theta^2 + \theta^4}$

      (b) is $\E\left[ (X_3 - P(X_3|\vect{W}))^2 \right] =
      \sigma^2 (1 + \theta^2) - \frac{\sigma^2\theta^2(1 + \theta^2)}{1 + \theta^2 + \theta^4}$

      (c) is $\E\left[ (X_3 - P(X_3|\vect{W}))^2 \right] =
      \sigma^2 (1 + \theta^2) - \frac{2\sigma^2\theta^2(1 + \theta^2)}{1 + \theta^2 + \theta^4}$
  \end{enumerate}
\end{proof}


% Problem 2.22
\begin{custompbm}{2.22}
  Repeat parts (a)-(d) of Problem 2.21 assuming now that the observations
  $X_1, X_2, X_3, X_4, X_5$ are from the causal AR(1) model
  \[
    X_t = \phi X_{t-1} + Z_t, \{Z_t\} \sim \text{WN}(0, \sigma^2)
  \]
\end{custompbm}

\begin{proof}
  If $Y$ and $W_n, \dots, W_1$ are random variables, then for $\vect{W} = (W_n, \dots, W_1)^\intercal$
  and $\vect{\mu}_W= \left(\E(W_n), \dots, \E(W_1)\right)^\intercal$, the best linear
  predictor of $Y$ in terms of $\{1, W_n, \dots, W_1\}$ is
  \[
    P(Y|\vect{W}) = \E(Y) + \vect{a}^\intercal (\vect{W} - \vect{\mu_W})
  \]
  where $\vect{a}$ is the solution of $\Gamma \vect{a} = \gamma$ for
  $\Gamma = \Co{\vect{W}}{\vect{W}}$ and $\gamma = \Co{Y}{\vect{W}}$.

  Also, note for an AR(1) process, the autocovariance function is defined as
  \[
    \gamma_X(h) = \frac{\sigma^2\phi^{|h|}}{1-\phi^2}
  \]
  \begin{enumerate}
    \item Using the above, set $Y = X_3$ and $W = (X_2, X_1)^\intercal$. Then
      \begin{align*}
        \Gamma = \Co{\vect{W}}{\vect{W}} =
        \begin{bmatrix} \gamma_X(0) & \gamma_X(1) \\ \gamma_X(1) & \gamma_X(0) \end{bmatrix} =
        \frac{\sigma^2}{1-\phi^2} \begin{bmatrix} 1  & \phi \\ \phi & 1 \end{bmatrix}
      \end{align*}
      and
      \begin{align*}
        \gamma = \begin{bmatrix} \gamma_X(1) \\ \gamma_X(2) \end{bmatrix}
        = \frac{\sigma^2}{1-\phi^2} \begin{bmatrix} \phi \\ \phi^2 \end{bmatrix}.
      \end{align*}
      The solution to the system of equations $\Gamma \vect{a} = \gamma$ is
      $$\vect{a} = \begin{bmatrix} \phi \\ 0\end{bmatrix}.$$
      Therefore, the best predictor of $X_3$ is
      \begin{align*}
        P(X_3|\vect{W}) &= \E(X_3) + \vect{a}^\intercal (\vect{W} - \vect{\mu}_W) = \phi X_2
      \end{align*}

    \item Using the above, set $Y = X_3$ and $W = (X_5, X_4)^\intercal$. Then
      \begin{align*}
        \Gamma = \Co{\vect{W}}{\vect{W}} =
        \begin{bmatrix} \gamma_X(0) & \gamma_X(1) \\ \gamma_X(1) & \gamma_X(0) \end{bmatrix} =
        \frac{\sigma^2}{1-\phi^2} \begin{bmatrix} 1  & \phi \\ \phi & 1 \end{bmatrix}
      \end{align*}
      and
      \begin{align*}
        \gamma = \begin{bmatrix} \gamma_X(2) \\ \gamma_X(1) \end{bmatrix}
        =  \frac{\sigma^2}{1-\phi^2} \begin{bmatrix} \phi^2 \\ \phi \end{bmatrix}.
      \end{align*}
      The solution to the system of equations $\Gamma \vect{a} = \gamma$ is
      $$\vect{a} = \begin{bmatrix} 0 \\ \phi \end{bmatrix}.$$
      Therefore, the best predictor of $X_3$ is
      \begin{align*}
        P(X_3|\vect{W}) &= \E(X_3) + \vect{a}^\intercal (\vect{W} - \vect{\mu}_W) = \phi X_4
      \end{align*}

    \item Using the above, set $Y = X_3$ and $W = (X_5, X_4, X_2, X_1)^\intercal$. Then
      \begin{align*}
        \Gamma = \Co{\vect{W}}{\vect{W}} &=
        \phantom{\frac{\sigma^2}{1-\phi^2} }
        \begin{bmatrix}
          \gamma_X(0) & \gamma_X(1) & \gamma_X(3) & \gamma_X(4) \\
          \gamma_X(1) & \gamma_X(0) & \gamma_X(2) & \gamma_X(3) \\
          \gamma_X(3) & \gamma_X(2) & \gamma_X(0) & \gamma_X(1) \\
          \gamma_X(4) & \gamma_X(3) & \gamma_X(1) & \gamma_X(0) \\
        \end{bmatrix}\\
        &=
        \frac{\sigma^2}{1-\phi^2}
        \begin{bmatrix}
          1 & \phi & \phi^3 & \phi^4 \\
          \phi & 1  & \phi^2 & \phi^3 \\
          \phi^3 & \phi^2 & 1  & \phi \\
          \phi^4 & \phi^3 & \phi & 1 \\
        \end{bmatrix}
      \end{align*}
      and
      \begin{align*}
        \gamma =
        \begin{bmatrix}
          \gamma_X(2) \\
          \gamma_X(1) \\
          \gamma_X(1) \\
          \gamma_X(2) \\
        \end{bmatrix}
        = \frac{\sigma^2}{1-\phi^2}
        \begin{bmatrix}
          \phi^2 \\
          \phi \\
          \phi \\
          \phi^2 \\
        \end{bmatrix}.
      \end{align*}
      The solution to the system of equations $\Gamma \vect{a} = \gamma$ is
      \begin{align*}
        \vect{a} = \phi
        \begin{bmatrix}
          0 \\
          \frac{1-\phi^2}{1-\phi^4} \\
          \frac{1-\phi^2}{1-\phi^4} \\
          0 \\
        \end{bmatrix}.
      \end{align*}
      Therefore, the best predictor of $X_3$ is
      \begin{align*}
        P(X_3|\vect{W}) &= \E(X_3) + \vect{a}^\intercal (\vect{W} - \vect{\mu}_W) \\
        &= \frac{\phi-\phi^3}{1-\phi^4}(X_4 + X_2)
      \end{align*}

    \item The mean squared error of the predictor in terms of the known random
      variables is $\E\left[ (Y - P(Y|\vect{W}))^2 \right] = \V(Y) - \vect{a}^\intercal \gamma$.

      Therefore, the mean squared error for:

      (a) is $\E\left[ (X_3 - P(X_3|\vect{W}))^2 \right] =
      \frac{\sigma^2}{1-\phi^2} - \frac{\sigma^2\phi^2}{1-\phi^2} = \sigma^2$

      (b) is $\E\left[ (X_3 - P(X_3|\vect{W}))^2 \right] =
      \frac{\sigma^2}{1-\phi^2} - \frac{\sigma^2\phi^2}{1-\phi^2} = \sigma^2$

      (c) is $\E\left[ (X_3 - P(X_3|\vect{W}))^2 \right] =
      \frac{\sigma^2}{1-\phi^2} - \frac{2\sigma^2\phi^2(1 - \phi^2)}{(1-\phi^2)(1-\phi^4)} = \frac{\sigma^2(1-\phi^2)}{1-\phi^4}$
  \end{enumerate}
\end{proof}


\end{document}

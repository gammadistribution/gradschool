\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amsthm, amssymb, amsfonts, breqn}


\theoremstyle{definition}
\newtheorem{problem}{Problem}
\renewcommand*{\proofname}{Solution}
\newenvironment{custompbm}[1]
  {\renewcommand\theproblem{#1}\problem}
  {\endproblem}
\renewcommand{\theenumi}{\alph{enumi}}


\newcommand{\E}{\text{E}}
\newcommand{\V}{\text{Var}}
\newcommand{\Co}[2]{\text{Cov}({#1}, {#2})}
\newcommand{\pdf}{\text{pdf}}
\newcommand{\pmf}{\text{pmf}}
\newcommand{\me}{\mathrm{e}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand{\vect}[1]{\boldsymbol{#1}}
\newcommand{\mx}[1][t]{\mu_X({#1})}
\newcommand{\gx}[2]{\gamma_X({#1}, {#2})}


\title{Homework Assignment 3}
\author{Matthew Tiger}


\begin{document}


\maketitle


% Problem 1.4
\begin{custompbm}{1.4}
  Let $\{ Z_t \}$ be a sequence of independent normal random variables, each with
  mean $0$ and variance $\sigma^2$, and let $a$, $b$, $c$ be constants. Which,
  if any, of the following processes are stationary? For each stationary process
  specify the mean and autocovariance function.
  \begin{enumerate}
    \item $X_t = a + bZ_t + cZ_{t-2}$
    \item $X_t = Z_1 \cos(ct) + Z_2 \sin(ct)$
    \item $X_t = Z_t \cos(ct) + Z_{t-1} \sin(ct)$
    \item $X_t = a + bZ_0$
    \item $X_t = Z_0\cos(ct)$
    \item $X_t = Z_tZ_{t-1}$
  \end{enumerate}
\end{custompbm}

\begin{proof}
  In the following we assume that $Z_t \sim N(0, \sigma^2)$.
  \begin{enumerate}
    \item The mean function of this process is given by
      \begin{align*}
        \mx &= \E(X_t) \\
        &= \E(a + bZ_t + cZ_{t-2}) \\
        &= a + b\E(Z_t) + c\E(Z_{t-2}) = a
      \end{align*}
      where $\E(Z_u) = 0$ since $Z_u \sim N(0, \sigma^2)$.

      Using the linearity of the covariance, the covariance function of this
      process is given by
      \begin{align*}
        \gx{t+h}{t} &= \Co{X_{t+h}}{X_t} \\
        &= \Co{a + bZ_{t + h} + cZ_{t + h - 2}} {a + bZ_t + cZ_{t-2}} \\
        &= \Co{bZ_{t + h}} {a + bZ_t + cZ_{t-2}} + \Co{cZ_{t + h - 2}} {a + bZ_t + cZ_{t-2}} \\
        &= b^2 \Co{Z_{t+h}}{Z_t} + bc\Co{Z_{t+h}}{Z_{t-2}} + bc\Co{Z_{t+h-2}}{Z_{t}} + c^2 \Co{Z_{t+h-2}}{Z_{t-2}}.
      \end{align*}
      The independence of the random variables $Z_u$ shows us that the autocovariance
      function is a function of $h$ and that
      \[
        \gamma_X(h) =
        \begin{cases}
          (b^2 + c^2)\sigma^2 & \text{if $h=0$}\\
          (bc)\sigma^2 & \text{if $h = \pm 2$} \\
          0 & \text{otherwise}
        \end{cases}.
      \]
      Since the mean function does not depend on $t$ and the covariance function does not depend on $t$
      for each $h$, this process is stationary.
    \item The mean function of this process is given by
      \begin{align*}
        \mx &= \E(X_t) \\
        &= \E(Z_1 \cos(ct)+ Z_2 \sin(ct))\\
        &= \E(Z_1 \cos(ct))+ \E(Z_2 \sin(ct)) \\
        &= \cos(ct)\E(Z_1)+ \sin(ct)\E(Z_2) = 0
      \end{align*}
      where $\E(Z_u) = 0$ since $Z_u \sim N(0, \sigma^2)$.

      Using the linearity of the covariance, the covariance function of this
      process is given by
      \begin{align*}
        \gx{t+h}{t} &= \Co{X_{t+h}}{X_t} \\
        &= \Co{Z_1 \cos(c(t+h))+ Z_2 \sin(c(t+h))} {Z_1 \cos(ct)+ Z_2 \sin(ct)} \\
        &= \cos(c(t+h))\cos(ct)\Co{Z_1}{Z_1} + \cos(c(t+h))\sin(ct)\Co{Z_1}{Z_2} \\
        & \hspace{5mm} + \sin(c(t+h))\cos(ct)\Co{Z_2}{Z_1} + \sin(c(t+h))\sin(ct)\Co{Z_2}{Z_2} \\
        &= \cos(c(t+h))\cos(ct) \sigma^2 + \sin(c(t+h))\sin(ct) \sigma^2 \\
        &= \sigma^2(\cos^2(ct)\cos(ch) - \sin(ct)\sin(ch)\cos(ct) \\
        &\hspace{5mm}+ \sin^2(ct)\cos(ch) + \cos(ct)\sin(ch)\sin(ct)) \\
        &= \sigma^2 \cos(ch)
      \end{align*}
      due to the independence of the random variables. Since the mean function
      does not depend on $t$ and the covariance function does not depend on $t$
      for each $h$, this process is stationary.
    \item The mean function of this process is given by
      \begin{align*}
        \mx &= \E(X_t) \\
        &= \E(Z_t \cos(ct)+ Z_{t-1} \sin(ct))\\
        &= \E(Z_t \cos(ct))+ \E(Z_{t-1} \sin(ct)) \\
        &= \cos(ct)\E(Z_t)+ \sin(ct)\E(Z_{t-1}) = 0
      \end{align*}
      where $\E(Z_u) = 0$ since $Z_u \sim N(0, \sigma^2)$.

      Using the linearity of the covariance, the covariance function of this
      process is given by
      \begin{align*}
        \gx{t+h}{t} &= \Co{X_{t+h}}{X_t} \\
        &= \Co{Z_{t+h} \cos(c(t+h))+ Z_{t+h-1} \sin(c(t+h))} {Z_t \cos(ct)+ Z_{t-1} \sin(ct)} \\
        &= \cos(c(t+h))\cos(ct)\Co{Z_{t+h}}{Z_t} + \cos(c(t+h))\sin(ct)\Co{Z_{t+h}}{Z_{t-1}} \\
        & \hspace{5mm} + \sin(c(t+h))\cos(ct)\Co{Z_{t+h-1}}{Z_t} + \sin(c(t+h))\sin(ct)\Co{Z_{t+h-1}}{Z_{t-1}}.
      \end{align*}
      The independence of the random variables shows that
      \begin{align*}
        \gamma_X(t + h, t) =
        \begin{cases}
          (\cos^2(ct) + sin^2(ct))\sigma^2 = \sigma^2 & \text{if $h=0$} \\
          \sin(c(t+1))\cos(ct)\sigma^2 & \text{if $h=1$} \\
          \cos(c(t-1))\sin(ct)\sigma^2 & \text{if $h=-1$} \\
          0 & \text{otherwise} \\
        \end{cases}.
      \end{align*}
      It is apparent that the covariance function depends on $t$ so this process
      is not stationary.
    \item The mean function of this process is given by
      \begin{align*}
        \mx &= \E(X_t) \\
        &= \E(a + bZ_0) = a + b\E(Z_0) = a
      \end{align*}
      where $\E(Z_0) = 0$ since $Z_0 \sim N(0, \sigma^2)$.

      It is clear that the covariance function is given by
      \begin{align*}
        \gx{t+h}{t} &= \Co{X_{t+h}}{X_t} \\
        &= \Co{a + b Z_0}{a + b Z_0}\\
        &= \Co{a}{a + b Z_0} + \Co{b Z_0}{a + b Z_0} \\
        &= \Co{b Z_0}{a} + \Co{b Z_0}{b Z_0} \\
        &= b^2 \Co{Z_0}{Z_0} = b^2 \sigma^2.
      \end{align*}
      Therefore, the autocovariance function is given by $\gamma_X(h) = b^2 \sigma^2$.
      As the covariance function does not depend on $t$ for any $h$ and the mean
      function does not depend on $t$, this process is stationary.
    \item The mean function of this process is given by
      \begin{align*}
        \mx &= \E(X_t) \\
        &= \E(Z_0 \cos(ct)) = \cos(ct)\E(Z_0) = 0
      \end{align*}
      where $\E(Z_0) = 0$ since $Z_0 \sim N(0, \sigma^2)$.

      The covariance function of this process is given by
      \begin{align*}
        \gx{t+h}{t} &= \Co{X_{t+h}}{X_t} \\
        &= \Co{Z_0\cos(c(t+h))}{Z_0\cos(ct)}\\
        &= \cos(c(t+h))\cos(ct)\Co{Z_0}{Z_0} = \cos(c(t+h))\cos(ct)\sigma^2.
      \end{align*}
      As the covariance function depends on $t$, this is not a stationary
      process.
    \item The mean function of this process is given by
      \begin{align*}
        \mx &= \E(X_t) \\
        &= \E(Z_t Z_{t-1}) = \E(Z_t)\E(Z_{t-1}) = 0
      \end{align*}
      where $\E(Z_u) = 0$ since $Z_u \sim N(0, \sigma^2)$ and
      $\E(Z_t Z_{t-1})=\E(Z_t)\E(Z_{t-1})$ due to the independence of the
      random variables.

      It is clear that the covariance function is given by
      \begin{align*}
        \gx{t+h}{t} &= \Co{X_{t+h}}{X_t} \\
        &= \E(X_{t+h} - \E(X_{t+h}))\E(X_t - \E(X_t)) \\
        &= \E(X_{t+h})\E(X_t) = \mu_X(t+h)\mu_X(t) = 0.
      \end{align*}
      Therefore the autocovariance function $\gamma_X(h) = 0$.
      As the mean function does not depend on $t$ and the autocovariance function
      does not depend on $t$ for any $h$, this process is stationary.
  \end{enumerate}
\end{proof}


% Problem 1.5
\begin{custompbm}{1.5}
  Let $\{ X_t \}$ be the moving-average process of order 2 given by
  \[
    X_t = Z_t + \theta Z_{t-2}
  \]
  where $\{ Z_t \}$ is $\text{WN}(0, 1).$
  \begin{enumerate}
    \item Find the autocovariance and autocorrelation functions for this process
      when $\theta = 0.8$.
    \item Compute the variance of the sample mean $(X_1 + X_2 + X_3 + X_4) / 4$
      when $\theta = 0.8$.
    \item Repeat (b) when $\theta = -0.8$ and compare your answer with the result
      obtained in (b).
  \end{enumerate}
\end{custompbm}

\begin{proof}
  For the following, let $\{ Z_t \}$ be $\text{WN}(0, 1).$
  \begin{enumerate}
    \item The covariance function for this process for any $\theta$ is given by
      \begin{align*}
        \gx{t+h}{t} &= \Co{X_{t+h}}{X_t} \\
        &= \Co{Z_{t+h} + \theta Z_{t + h - 2}}{Z_t + \theta Z_{t-2}} \\
        &= \Co{Z_{t+h}}{Z_t} + \theta \Co{Z_{t+h}}{Z_{t-2}} + \theta \Co{Z_{t+h-2}}{Z_t} + \theta^2 \Co{Z_{t+h-2}}{Z_{t-2}}
      \end{align*}
      Since $\{ Z_t \}$ is $\text{WN}(0, 1),$ the random variables are independent
      and the autocovariance function is given by
      \begin{align*}
        \gamma_X(h) =
        \begin{cases}
          1 + \theta^2 & \text{if $h=0$} \\
          \theta & \text{if $h =\pm2$} \\
          0 & \text{otherwise}
        \end{cases}
      \end{align*}
      Knowing the autocorrelation function, we know that the autocovariance function is
      \begin{align*}
        \rho_X(h) = \frac{\gamma_X(h)}{\gamma_X(0)} =
        \begin{cases}
          1 & \text{if $h=0$} \\
          \frac{\theta}{1 + \theta^2} & \text{if $h =\pm2$} \\
          0 & \text{otherwise}
        \end{cases}
      \end{align*}
      Substituting $\theta = 0.8$ will reveal the desired autocovariance and
      autocorrelation functions.
    \item Let the sample mean be defined as $\bar{x} = (X_1 + X_2 + \dots + X_n) / n$.
      Then the variance of $\bar{x}$ is given by
      \begin{align*}
        \V(\bar{x}) &= \Co{\bar{x}}{\bar{x}} \\
        &= \frac{1}{n^2}\text{Cov}\left(\sum_{i=1}^n X_i, \sum_{i=1}^n X_i\right) \\
        &= \frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n\Co{X_i}{X_j} \\
        &= \frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n \gx{i}{j} \\
      \end{align*}
      where
      \begin{align*}
        \gamma_X(i,j) =
        \begin{cases}
          1 + \theta^2 & \text{if $i=j$} \\
          \theta & \text{if $i = j +2$ or $i = j -2$} \\
          0 & \text{otherwise}
        \end{cases}.
      \end{align*}
      Using this covariance function we know that $\gamma_X(i,j) = 0$ if $i \neq j$ or $i$ does not differ from $j$ by 2, so that we can partition the sum as
      \begin{align*}
        \frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n \gx{i}{j}
        &= \frac{1}{n^2}\left(\sum_{k=1}^n\gx{k}{k} + \sum_{k=1}^{n-2}\gx{k}{k + 2} + \sum_{k=3}^{n}\gx{k}{k - 2}\right) \\
        &= \frac{1}{n^2}\left(n(1+\theta^2) + (n-2)\theta + (n-2)\theta\right) \\
        &= \frac{n(1 + \theta^2) + 2(n-2)\theta}{n^2}
      \end{align*}
      Therefore, $\V(\bar{x}) = (n(1 + \theta^2) + 2(n-2)\theta) / n^2$. As we wish to know the
      variance of the sample mean $\bar{x} = (X_1 + X_2 + X_3 + X_4) / 4$, we can replace $n$ with 4
      and $\theta$ with $0.8$ so that $\V(\bar{x}) = 0.61$.
    \item Using the formula derived in the previous problem with $\theta = -0.8$
      and $n=4$, it is easy to see that $\V(\bar{x}) = 0.21$.
  \end{enumerate}
\end{proof}


% Problem 1.6
\begin{custompbm}{1.6}
  Let $\{ X_t \}$ be the $\text{AR}(1)$ process defined in Example 1.4.5.
  \begin{enumerate}
    \item Compute the variance of the sample mean $(X_1 + X_2 + X_3 + X_4) / 4$
      when $\phi = 0.9$ and $\sigma^2 = 1$.
    \item Repeat (a) when $\phi = -0.9$ and compare your answer with the result
      obtained in (a).
  \end{enumerate}
\end{custompbm}

\begin{proof}
  \begin{enumerate}
    \item Let the sample mean be defined as $\bar{x} = (X_1 + X_2 + \dots + X_n) / n$.
      We know that autocovariance function is given by
      $\gamma_X(h) = (\sigma^2\phi ^ {|h|} ) / (1 - \phi^2)$ using Example 1.4.5.
      Note that,
      \begin{align*}
        \V(\bar{x}) &= \Co{\bar{x}}{\bar{x}} \\
        &= \frac{1}{n^2}\text{Cov}\left(\sum_{i=1}^n X_i, \sum_{i=1}^n X_i\right) \\
        &= \frac{1}{n^2}\sum_{i=1}^{n} \sum_{j=1}^n \gamma_X(i,j) \\
        &= \frac{1}{n^2}\sum_{h=-n}^{n} (n - |h|)\gamma_X(h)\\
        &= \frac{1}{n}\left[\gamma_X(0) + 2\sum_{h=1}^{n}\left(1-\frac{h}{n}\right)\gamma_X(h)\right].
      \end{align*}
      Using the autocovariance function $\gamma_X(h)$, we can see that
      \begin{align*}
        \V(\bar{x}) &= \frac{1}{n}\left[\gamma_X(0) + 2\sum_{h=1}^{n}\left(1-\frac{h}{n}\right)\gamma_X(h)\right] \\
        &= \frac{1}{n}\left[\frac{\sigma^2}{1-\phi^2} + \frac{2\sigma^2}{1-\phi^2}\sum_{h=1}^n \left(1-\frac{h}{n}\right)\phi^h \right] \\
        &= \frac{\sigma^2}{n(1-\phi^2)}\left[1 + 2\left(\sum_{h=1}^n \phi^h- \frac{1}{n}\sum_{h=1}^n h\phi^h \right)\right] \\
        &= \frac{\sigma^2}{n(1-\phi^2)}\left[1 + 2\left(\frac{\phi-\phi^{n+1}}{1-\phi}- \frac{(n\phi - n - 1)\phi^{n+1} + \phi}{n(1-\phi)^2}\right)\right] \\
      \end{align*}
      Now, when $\phi = 0.9$ and $\sigma^2 = 1$, we set $n=4$ and $\V(\bar{x}) = 4.6375$.
    \item Using the formula above with $\phi = -0.9$ and $\sigma^2 = 1$, it is
      clear that for sample mean $\bar{x} = (X_1 + X_2 + X_3 + X_4) / 4$, $\V(\bar{x}) = 0.1257$.
  \end{enumerate}
\end{proof}


% Problem 1.7
\begin{custompbm}{1.7}
  If $\{ X_t \}$ and $\{ Y_t \}$ are uncorrelated stationary sequences, i.e.,
  if $X_r$ and $Y_s$ are uncorrelated for every $r$ and $s$, show that
  $\{ X_t + Y_t \}$ is stationary with autocovariance function equal to the sum
  of the autocovariance functions of $\{ X_t \}$ and $\{ Y_t \}$.
\end{custompbm}

\begin{proof}
  Define $\{Z_t\} = {X_t} + {Y_t}$. We wish to prove that $\{Z_t\}$ is a
  stationary process.   If $\mu_X(t)$ and $\mu_Y(t)$ are the mean functions of $\{ X_t \}$ and $\{ Y_t \}$,
  respectively, then the mean function of this new process is
  \begin{align*}
    \mu_Z(t) &= \E(Z_t) = \E(X_t + Y_t) \\
    &= \E(X_t) + \E(Y_t) = \mu_X(t) + \mu_Y(t).
  \end{align*}
  Since $\{ X_t \}$ and $\{ Y_t \}$ are stationary processes, $\mu_X(t)$ and $\mu_Y(t)$ do
  not depend on $t$, thus their sum does not depend on $t$. Hence, the mean
  function of $\{Z_t\}$ does not depend on $t$.

  If $\gamma_X(h)$ and $\gamma_Y(h)$ are the autocovariance functions of $\{ X_t \}$ and $\{ Y_t \}$,
  respectively, then the covariance function of $\{ Z_t \}$ is
  \begin{align*}
    \gamma_Z(t+h, t) &= \Co{Z_{t+h}}{Z_t} \\
    &= \Co{X_{t+h} + Y_{t+h}}{X_{t} + Y_{t}} \\
    &= \Co{X_{t+h}}{X_{t} + Y_{t}} + \Co{Y_{t+h}}{X_{t} + Y_{t}} \\
    &= \Co{X_{t+h}}{X_t} + \Co{X_{t+h}}{Y_t} + \Co{Y_{t+h}}{X_t} + \Co{Y_{t+h}}{Y_t} \\
    &= \Co{X_{t+h}}{X_t} + \Co{Y_{t+h}}{Y_t} = \gamma_X(h) + \gamma_Y(h)
  \end{align*}
  due to the fact that $X_r$ and $Y_s$ are uncorrelated for any $r$ or $s$. Since
  $\{ X_t \}$ and $\{ Y_t \}$ are stationary processes, $\gamma_X(h)$ and $\gamma_Y(h)$ do
  not depend on $t$ for any $h$, thus their sum does not depend on $t$ for any $h$. Hence, the covariance
  function of $\{Z_t\}$ does not depend on $t$ for any $h$ and the autocovariance
  function is $\gamma_Z(h) = \gamma_X(h) + \gamma_Y(h)$.

  Since the mean function of $\{Z_t\}$ does not depend on $t$ and the autocovariance
  function does not depend on $t$ for any $h$, this process is stationary.
\end{proof}


\end{document}

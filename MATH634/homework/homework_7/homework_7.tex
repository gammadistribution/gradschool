\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amsthm, amssymb, amsfonts, breqn}


\theoremstyle{definition}
\newtheorem{problem}{Problem}
\renewcommand*{\proofname}{Solution}
\newenvironment{custompbm}[1]
  {\renewcommand\theproblem{#1}\problem}
  {\endproblem}
\renewcommand{\theenumi}{\alph{enumi}}


\newcommand{\E}{\text{E}}
\newcommand{\V}{\text{Var}}
\newcommand{\Co}[2]{\text{Cov}({#1}, {#2})}
\newcommand{\pdf}{\text{pdf}}
\newcommand{\pmf}{\text{pmf}}
\newcommand{\me}{\mathrm{e}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand{\vect}[1]{\boldsymbol{#1}}
\newcommand{\mx}[1][t]{\mu_X({#1})}
\newcommand{\gx}[2]{\gamma_X({#1}, {#2})}


\title{Homework Assignment 7}
\author{Matthew Tiger}


\begin{document}


\maketitle



% Problem 3.2
\begin{custompbm}{3.2}
  For those processes in Problem 3.1 that are causal, compute and graph their
  ACF and PACF using the program ITSM.
\end{custompbm}

\begin{proof}
  From problem 3.1, the following processes are causal:
  \begin{enumerate}
    \item $X_t - (-0.2)X_{t-1} - 0.48X_{t-2} = Z_t$
    \item $X_t - (-0.6)X_{t-1} = Z_t + 1.2Z_{t-1}$
    \item $X_t - (-1.8)X_{t-1} - (-0.8)1X_{t-2} = Z_t$.
  \end{enumerate}

  I am unable to copy the graphs from the program and save the image data.

  To create one such graph, open itsm.exe and follow the below steps.
  \begin{enumerate}
    \item Create a new univariate project.
    \item Go to \texttt{Model > Specify}.
    \item Specify the AR order and MA order and enter in the coefficients to the above processes.
      \item Go to \texttt{Model > ACF/PACF > Model} to display the graph.
  \end{enumerate}
  These instructions can be repeated for each model to create the three graphs
\end{proof}


% Problem 3.4
\begin{custompbm}{3.4}
  Compute the ACF and PACF of the AR(2) process
  \[
    X_t = 0.8X_{t-2} + Z_t, \quad \{Z_t\} \sim \text{WN}(0, \sigma ^2)
  \]
\end{custompbm}

\begin{proof}
  This process is equivalently written as
  \[
    X_t - 0X_{t-1} - 0.8X_{t-2} = Z_t.
  \]
  Thus, $\phi_0 = 1$, $\phi_1 = 0$, $\phi_2 = 0.8$, and $\phi_k = 0$ for $k > 2$
  and $\theta_0 = 1$, $\theta_k = 0$ for $k > 0$. The ACF, $\rho(h)$, is defined
  as
  \[
    \rho(h) = \frac{\gamma(h)}{\gamma(0)}
  \]
  where $\gamma(h) = \sigma^2 \sum_{j=0}^\infty \psi_j \psi_{j+|h|}$ and $\psi_0 = 1$,
  $\psi_j = \theta_j + \sum_{k=1}^p \phi_k \psi_{j-k}$ for $j >1$ and $p=2$.

  Using the coefficients $\phi_k$ and $\theta_k$, we see that
  \[
    \psi_j = \phi_1 \psi_{j-1} + \phi_2 \psi_{j-2} = 0.8\psi_{j-2}
  \]
  where $\psi_j = 0$ if $j<0$.

  If $j = 2k+1$ for $k\geq 0$, then $\psi_j = 0$ and if $j = 2k$ for $k\geq 0$,
  then $\psi_j = 0.8^k$. These two formulations are easily proved via induction.

  Now,
  \begin{align*}
    \gamma(h) = \sigma^2 \sum_{j=0}^\infty \psi_j \psi_{j + |h|}
    &= \sigma^2 \left(\sum_{k=0}^\infty \psi_{2k} \psi_{2k + |h|} + \sum_{j=0}^\infty \psi_{2k+1} \psi_{2k+1 + |h|}\right) \\
    &= \sigma^2 \sum_{k=0}^\infty \psi_{2k} \psi_{2k + |h|}
  \end{align*}
  since $\psi_j = 0$ for even $j$.

  If $h = 2l + 1$, then $\gamma(h) = \sigma^2 \sum_{k=0}^\infty \psi_{2k} \psi_{2k + |2l+1|} = 0$
  since $\psi_{2k + |2l+1|} = 0$. If $h =2l$, then
  \begin{align*}
    \gamma(h) = \sigma^2 \sum_{k=0}^\infty \psi_{2k} \psi_{2(k+ |l|) + 1} = \sigma^2 (0.8)^{|l|} \sum_{k=0}^\infty (0.8^2)^k = \frac{\sigma^2(0.8^{|l|})}{0.36}
  \end{align*}
  Therefore,
  \[
  \rho(h) =
  \begin{cases}
    0 & \text{if $h = 2l + 1$}\\
    (0.8)^{|l|} & \text{if $h = 2l$}
  \end{cases}
  \]

  For an AR($p$) process, for the PACF function $\alpha(h)$,
  \[
    \alpha(p) = \phi_p \quad \text{and} \quad \alpha(h) = 0 \ \text{if $h>p$}.
  \]
  Thus, we need only compute $\alpha(1)$. Now, $\alpha(1) = \gamma(1) / \gamma(0) = 0$.
  Therefore,

  \[
  \alpha(h) =
  \begin{cases}
    1 & \text{if $h = 0$}\\
    0.8 & \text{if $h = 2$}\\
    0 & \text{otherwise}\\
  \end{cases}.
  \]
\end{proof}


% Problem 3.6
\begin{custompbm}{3.6}
  Show that the two MA(1) processes
  \begin{align*}
    X_t &= Z_t + \theta Z_{t-1}, \quad \{Z_t\} \sim \text{WN}(0, \sigma ^ 2)\\
    Y_t &= \widetilde{Z}_t + \theta \widetilde{Z}_{t-1}, \quad \{\widetilde{Z_t}\} \sim \text{WN}(0, \sigma ^ 2)
  \end{align*}
  where $0 < |\theta| < 1$, have the same autocovariance functions.
\end{custompbm}

\begin{proof}
  Note that for an ARMA$(p,q)$ the autocovariance function $\gamma(h) = \sigma^2 \sum_{j=0}^\infty \psi_j \psi_{j+|h|}$
  where $\psi_0 = 1$, $\psi_j = \theta_j + \sum_{k=1}^p \phi_k \psi_{j-k}$ for $j >1$ and $\psi_j = 0$ for $j<0$.

  For $X_t$, we have
  \[
    \psi_j =
    \begin{cases}
      1 & \text{ if $j = 0$}\\
      \theta & \text{if $j = 1$}\\
      0 & \text{if $j > 1$}
    \end{cases}
  \]
  and for $Y_t$, we have
  \[
    \widetilde{\psi}_j =
    \begin{cases}
      1 & \text{ if $j = 0$}\\
      \frac{1}{\theta} & \text{if $j = 1$}\\
      0 & \text{if $j > 1$}
    \end{cases}
  \]
  Thus, $$\gamma_X(h) = \sigma^2 \sum_{j=0}^\infty \psi_{j} \psi_{j + |h|} = \sigma^2 \sum_{j=0}^1 \psi_{j} \psi_{j + |h|} = \sigma^2(\psi_{|h|} + \theta\psi_{1+|h|})$$
  and $$\gamma_Y(h) = \sigma^2\theta^2 \sum_{j=0}^\infty \widetilde{\psi}_{j} \widetilde{\psi}_{j + |h|} = \sigma^2\theta^2 \sum_{j=0}^1 \widetilde{\psi}_{j} \widetilde{\psi}_{j + |h|}= \sigma^2\theta^2\left(\psi_{|h|} + \frac{1}{\theta}\psi_{1+|h|}\right ).$$
  Explicitly,
  \[
    \gamma_X(h) =
    \begin{cases}
      \sigma^2(1+\theta^2) & \text{ if $h = 0$}\\
      \sigma^2 \theta & \text{if $|h| = 1$}\\
      0 & \text{if $|h| > 1$}
    \end{cases}
  \]
  and
  \[
    \gamma_X(h) =
    \begin{cases}
      \sigma^2\theta^2\left(1+\frac{1}{\theta^2}\right) & \text{ if $h = 0$}\\
      \sigma^2 \theta^2 \left(\frac{1}{\theta}\right) & \text{if $|h| = 1$}\\
      0 & \text{if $|h| > 1$}
    \end{cases}.
  \]
  Therefore, these two processes autocovariance functions are the same.
\end{proof}


% Problem 3.10
\begin{custompbm}{3.10}
  As defined in the book.
\end{custompbm}

\begin{proof}
  We want to fit the \texttt{strikes.tsm} data to the mean corrected model
  $Y_t - \phi Y_{t-1} = Z_t$ where $Y_t = X_t - \mu$ and $X_t$ is the data point
  from \texttt{strikes.tsm} at time $t$. Note that this is an AR(1) model.
  As such we know that the AR(1) autocovariance function is given by
  \[
    \gamma_Y(h) = \sigma^2 \sum_{j=0} ^ \infty \psi_{j} \psi_{j + |h|} = \sigma^2 \phi^{|h|} \sum_{j=0}^\infty \phi^j = \frac{\sigma^2 \phi^{|h|}}{1-\phi^2}
  \]
  for $|\phi| < 1$.

  From the ITSM tool, we know that the sample variance is $\hat{\gamma}(0) = 676789$.
  Using the fact that the sample ACF at lag 1 is $\hat{\rho}(1) = .7323$, we know
  that $\hat{\gamma}(1) = \hat{\rho}(1)\hat{\gamma}(0) = 495612.5847$.

  Equating $\gamma_Y(h)$ to $\hat{\gamma}(h)$ at lags 0 and 1 gives us two equations
  to solve for the unknown parameters $\phi$ and $\sigma^2$.

  Writing out the equations and choosing $\phi$ so that $|\phi| < 1$ gives
  $phi=0.5646$ and $\sigma^2 = 461050$. These parameters define the given model
  and we are done.
\end{proof}


\end{document}

\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amsthm, amssymb, amsfonts, enumitem, graphicx}
\usepackage{nth}
\usepackage{fancyhdr}
\usepackage{tikz}
\usepackage{mathrsfs}
\usepackage{mathtools}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}

\theoremstyle{definition}
\newtheorem{problem}{Problem}
\renewcommand*{\proofname}{Solution}
\renewcommand{\theenumi}{\alph{enumi}}

\newcommand{\vc}[1]{\boldsymbol{#1}}

\newenvironment{amatrix}[1]{%
  \left[\begin{array}{@{}*{#1}{c}|c@{}}
}{%
  \end{array}\right]
}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\grstep}[2][\relax]{%
   \ensuremath{\mathrel{
       {\mathop{\longrightarrow}\limits^{#2\mathstrut}_{
                                     \begin{subarray}{l} #1 \end{subarray}}}}}}
\newcommand{\swap}{\leftrightarrow}
\newcommand{\tran}{\mathsf{T}}
\newtheorem{theorem}{Theorem}



\pagestyle{fancy}
\fancyhf{}
\rhead{Homework Assignment 8}
\lhead{Matthew Tiger}
\cfoot{\thepage}


\title{Homework Assignment 8}
\author{Matthew Tiger}


\begin{document}


\maketitle


% Problem 1
\begin{problem}
  Write at least two necessary conditions and at least two sufficient conditions for
  a function $f: \mathbb{R}^n \to \mathbb{R}$ to be concave.
\end{problem}

\begin{proof}
  By definition, a function $f: \mathbb{R}^n \to \mathbb{R}$ is concave over the convex
  set $\Omega \subset \mathbb{R}^n$ if $-f$ is convex over $\Omega$. This
  definition will allow us to obtain results for concave functions by replacing
  $f$ with $-f$ in previously obtained results concerning convex functions.

  Using this definition and Theorem 22.2, we see that the condition that if for all
  $\alpha \in (0, 1)$ and for all $\vc{x}, \vc{y} \in \Omega$, we have that
  \begin{align*}
    f(\alpha\vc{x} + (1-\alpha)\vc{y}) \geq \alpha f(\vc{x}) + (1-\alpha)f(\vc{y})
  \end{align*}
  is a necessary and sufficient condition for $f$ to be concave on the convex set $\Omega$.

  Further, if the function $f$ is $\mathcal{C}^1$-smooth, we see from the above definition and Theorem 22.4 that the condition that
  if for all $\vc{x}, \vc{y} \in \Omega$, we have that
  \begin{align*}
    f(\vc{x}) \leq f(\vc{y}) + D f(\vc{x}) (\vc{x} - \vc{y})
  \end{align*}
  is a necessary and sufficient condition for $f$ to be concave on the open convex set $\Omega$.

  Going one last step further, if the function $f$ is $\mathcal{C}^2$-smooth, we see from the above definition and Theorem
  22.5 that the condition that if for all $\vc{x}\in\Omega$, the Hessian matrix $\vc{F}(\vc{x})$ of $f$ at $\vc{x}$ is a negative semi-definite matrix
  is a necessary and sufficient condition for $f$ to be concave on the open convex set $\Omega$.
\end{proof}
\newpage


% Problem 2
\begin{problem}
  Let $S \subset \mathbb{R}^n$ be a convex set and let $\vc{x}^* \in S$. Prove that a vector
  $\vc{d} \in \mathbb{R}^n$ is a feasible direction at $\vc{x}^*$ (relative to $S$) if and only
  there exists $t_0 > 0$ such that $\vc{x}^* + t_0 \vc{d} \in S$ with $\vc{d} \neq \vc{0}$.
\end{problem}

\begin{proof}
  Suppose first that the vector $\vc{d} \in \mathbb{R}^n$ is a feasible
  direction at $\vc{x}^*$ (relative to $S$). By definition, the vector
  $\vc{d}$ is a feasible direction at $\vc{x}^*\in S$ if there exists
  $t_0 > 0$ such that $\vc{x}^* + t \vc{d} \in S$ for all $t \in [0, t_0]$
  with $\vc{d} \neq \vc{0}$. Thus, choosing $t=t_0$, we have by the above definition that there
  exists $t_0 > 0$ such that $\vc{x}^* + t_0\vc{d} \in S$ with $\vc{d} \neq \vc{0}$, proving the first implication.

  Now suppose that there exists $t_0 > 0$ such that $\vc{x}^* + t_0 \vc{d} \in S$
  with $\vc{d} \neq \vc{0}$. Since $S$ is convex and $\vc{x}^* \in S$, any convex combination
  of $\vc{x}^*$ and $\vc{x}^* + t_0 \vc{d}$ will also be in $S$, i.e.\ for all
  $\alpha \in [0, 1]$, we have that
  \begin{align*}
    \alpha \vc{x}^* + (1-\alpha)(\vc{x}^* + t_0 \vc{d}) = \vc{x}^* + (1-\alpha)t_0 \vc{d} \in S.
  \end{align*}
  Since $t_0 > 0$, we have that the following two sets are equal:
  \begin{align*}
    \{(1-\alpha)t_0 \ | \ 0 \leq \alpha \leq 1\} = \{ t \ |\ 0 \leq t \leq t_0\}.
  \end{align*}
  Thus, if $\vc{x}^* + (1-\alpha)t_0 \vc{d} \in S$ for all $\alpha \in [0, 1]$, then
  $\vc{x}^* + t \vc{d} \in S$ for all $t \in [0, t_0]$. Therefore, if
  $\vc{x}^* \in S$ with $S$ a convex set and there exists $t_0 > 0$ such that
  $\vc{x}^* + t_0 \vc{d} \in S$ with $\vc{d} \neq \vc{0}$, then
  $\vc{x}^* + t \vc{d} \in S$ for all $t \in [0, t_0]$, i.e.\
  the vector $\vc{d}$ is a feasible direction at $\vc{x}^*$ (relative to $S$).
\end{proof}
\newpage


% Problem 3
\begin{problem}
  Recall that
  \begin{align*}
    \max\{\alpha, \beta\} :=
    \begin{cases}
      \alpha & \text{if $\alpha \geq \beta$} \\
      \beta & \text{if $\alpha < \beta$} \\
    \end{cases}.
  \end{align*}
  Given two convex functions $f_1: \mathbb{R}^n \to \mathbb{R} \cup \{+\infty\}$
  and $f_2: \mathbb{R}^n \to \mathbb{R} \cup \{+\infty\}$, prove that for
  $\vc{x} \in \mathbb{R}^n$, the function
  \begin{align*}
    f(\vc{x}) := \max\{f_1(\vc{x}), f_2(\vc{x})\}
  \end{align*}
  is convex.
\end{problem}

\begin{proof}
  Note that it is clear that the set $\mathbb{R}^n$ is convex. Therefore, the
  function $f(\vc{x}) := \max\{f_1(\vc{x}), f_2(\vc{x})\}$ is convex if for all
  $\alpha \in (0, 1)$ and for all $\vc{x}, \vc{y} \in \mathbb{R}^n$ we have that
  \begin{align}\label{convex_ineq}
    f(\alpha\vc{x} + (1-\alpha)\vc{y}) \leq \alpha f(\vc{x}) + (1-\alpha)f(\vc{y}).
  \end{align}

  If either $f(\vc{x}) = +\infty$ or $f(\vc{y}) = +\infty$, then for all
  $\alpha \in (0, 1)$ and for all $\vc{x}, \vc{y} \in \mathbb{R}^n$ we see that
  $\alpha f(\vc{x}) + (1-\alpha)f(\vc{y}) = + \infty$ and inequality \eqref{convex_ineq}
  holds showing the convexity of $f$ in this case.

  Now suppose that both $f(\vc{x})$ and $f(\vc{y})$ are finite. Without loss of generality,
  we may assume that at the point $\alpha\vc{x} + (1-\alpha)\vc{y}$, we have that
  the max of $f_1$ and $f_2$ at that point occurs at $f_1$, i.e.\
  \begin{align*}
    f(\alpha\vc{x} + (1-\alpha)\vc{y})
    &= \max\{f_1(\alpha\vc{x} + (1-\alpha)\vc{y}), f_2(\alpha\vc{x} + (1-\alpha)\vc{y})\} \\
    &= f_1(\alpha\vc{x} + (1-\alpha)\vc{y}).
  \end{align*}
  Then, due to the convexity of $f_1$, we have that for all $\alpha \in (0, 1)$ and for all $\vc{x}, \vc{y} \in \mathbb{R}^n$,
  \begin{align*}
    f_1(\alpha\vc{x} + (1-\alpha)\vc{y}) \leq \alpha f_1(\vc{x}) + (1-\alpha)f_1(\vc{y}).
  \end{align*}
  From the above definition of max, we readily see that
  \begin{align*}
    \alpha f_1(\vc{x}) + (1-\alpha)f_1(\vc{y})
    &\leq \alpha \max\{f_1(\vc{x}), f_2(\vc{x})\} + (1-\alpha)\max\{f_1(\vc{y}), f_2(\vc{y})\} \\
    &=\alpha f(\vc{x}) + (1-\alpha) f(\vc{y}).
  \end{align*}
  Therefore, combining, we have that for all $\alpha \in (0, 1)$ and for all $\vc{x}, \vc{y} \in \mathbb{R}^n$,
  \begin{align*}
    f(\alpha\vc{x} + (1-\alpha)\vc{y}) = f_1(\alpha\vc{x} + (1-\alpha)\vc{y}) \leq \alpha f(\vc{x}) + (1-\alpha) f(\vc{y})
  \end{align*}
  showing that inequality \eqref{convex_ineq} holds and that the function $f$ is convex.
\end{proof}
\newpage


% Problem 4
\begin{problem}
  Consider the pair of linear programming problems in asymmetric duality:
  \begin{align*}
    \begin{array}{ll}
      \begin{array}{rrl}
        (P_a) & \text{minimize} & f(\vc{x}) = \vc{c}^\tran\vc{x} \\
        & \text{subject to} & A\vc{x} = \vc{b} \\
        & & \vc{x} \geq \vc{0} \\
      \end{array}
      &
      \begin{array}{rrl}
        (D_a) & \text{maximize} & F(\vc{\lambda}) = \vc{\lambda}^\tran\vc{b} \\
        & \text{subject to} & \vc{\lambda}^\tran A \leq \vc{c}^\tran \\
        & & \\
      \end{array}
    \end{array}
  \end{align*}
  \begin{enumerate}
    \item Prove that $(D_a)$ is a convex programming problem.
    \item Write the KKT conditions for $(D_a)$.
    \item Suppose that $\vc{x}^*$ is feasible for $(P_a)$ and $\vc{\lambda}^*$ is
      feasible for $(D_a)$. Use the KKT conditions to prove that if $(\vc{c}^\tran - \vc{\lambda}^{*\tran}A)\vc{x}^* = 0$,
      then $\vc{\lambda}^*$ is optimal for $(D_a)$.
  \end{enumerate}
\end{problem}

\begin{proof}
  For the problem above, we assume that $\vc{x} \in \mathbb{R}^n$ and that $A$ is an $m \times n$
  matrix with $m < n$. This implies that $\vc{\lambda} \in \mathbb{R}^m$.
  \begin{enumerate}
    \item Note that $(D_a)$ is a convex programming problem if the
      constraint set
      $$\Omega = \{\vc{\lambda} \in \mathbb{R}^m \ |\ \vc{\lambda}^\tran A \leq \vc{c}^\tran\}$$
      is a convex set and if $F: \Omega \to \mathbb{R}^m \cup \{+\infty\}$ where
      $F(\vc{\lambda}) := \vc{\lambda}^\tran \vc{b}$ is a convex function.

      It is straight-forward to show that $\Omega$ is a convex set. Let $\vc{x}, \vc{y} \in \Omega$ be given.
      Then $\vc{x}^\tran A \leq \vc{c}^\tran, \vc{y}^\tran A \leq \vc{c}^\tran$ and for all $\alpha \in [0,1]$
      \begin{align*}
        \alpha \vc{x}^\tran A \leq \alpha \vc{c}^\tran \quad \text{and} \quad (1 -\alpha)\vc{y}^\tran A \leq (1-\alpha)\vc{c}^\tran.
      \end{align*}
      Thus, for all $\alpha \in [0, 1]$
      \begin{align*}
        (\alpha\vc{x}^\tran + (1-\alpha)\vc{y}^\tran)A = \alpha \vc{x}^\tran A + (1-\alpha)\vc{y}^\tran A \leq \alpha \vc{c}^\tran + (1-\alpha)\vc{c}^\tran = \vc{c}^\tran,
      \end{align*}
      i.e.\ the convex combination $\alpha\vc{x} + (1-\alpha)\vc{y} \in \Omega$ and that $\Omega$ is a convex set.

      Since $F$ is a linear function, it is of course convex showing that the problem $(D_a)$ is a convex programming problem.
    \item The KKT conditions for the convex programming problem $(D_a)$ are stated below:

      If the function $F \in \mathcal{C}^1$ is a convex function on the convex set of feasible points
      \begin{align*}
        \Omega = \{\vc{\lambda}\in \mathbb{R}^m\ |\ \vc{\lambda}^\tran A \leq \vc{c}^\tran \} = \{\vc{\lambda} \in \mathbb{R}^m \ | \ \vc{g}(\vc{\lambda}) = A^\tran \vc{\lambda} - \vc{c} \leq \vc{0}\}
      \end{align*}
      where $\vc{g} \in \mathcal{C}^1$ and
      if there exists $\vc{\lambda}^* \in \Omega$, $\vc{\mu}^* \in \mathbb{R}^n$ such that
      \begin{enumerate}[label = \roman*]
        \item $\vc{\mu}^* \geq \vc{0}$.
        \item $-D F(\vc{\lambda}^*) + \vc{\mu}^{*\tran}D \vc{g}(\vc{\lambda}^*) = -\vc{b}^\tran + \vc{\mu}^{*\tran}A^\tran = \vc{0}^\tran$.
        \item $\vc{\mu}^{*\tran}\vc{g}(\vc{\lambda}) = \vc{\mu}^{*\tran} (A^\tran \vc{\lambda}^* - \vc{c})= (\vc{\lambda}^{*\tran} A - \vc{c}^\tran)\vc{\mu}^{*}  = 0$.
      \end{enumerate}
      then $\vc{\lambda}^*$ is a global maximizer of $F$ over $\Omega$.
    \item Suppose that $\vc{x}^*$ is feasible for $(P_a)$ and $\vc{\lambda}^*$
      is feasible for $(D_a)$ and that $(\vc{c}^\tran - \vc{\lambda}^{*\tran}A)\vc{x}^* = 0$. Since $\vc{\lambda}^*$ is feasible for $(D_a)$, we know that $\vc{\lambda}^* \in \Omega$.
      Choose $\vc{\mu}^* = \vc{x}^*$. Then $\vc{\mu}^*$ satisfies the KKT conditions above, which we will now demonstrate.

      Since $\vc{x}^*$ is feasible for $(P_a)$, condition i.\ is readily seen to be true and since $A\vc{x}^* = \vc{b}$ or $\vc{b} - A\vc{x}^* = \vc{0}$, we see that
      \begin{align*}
        \vc{0}^\tran = \vc{b}^{*\tran} - (A\vc{x}^*)^\tran = \vc{b}^{*\tran} - \vc{x}^{*\tran}A^\tran = \vc{b}^{*\tran} - \vc{\mu}^{*\tran}A^\tran
      \end{align*}
      or that $-\vc{b}^{*\tran} + \vc{\mu}^{*\tran}A^\tran = \vc{0}^\tran$ and condition ii.\ is satisfied.
      Since by assumption we have that $(\vc{c}^\tran - \vc{\lambda}^{*\tran}A)\vc{x}^* = (\vc{c}^\tran - \vc{\lambda}^{*\tran}A)\vc{\mu}^* =0$, we see
      also that $(\vc{\lambda}^{*\tran}A - \vc{c}^\tran)\vc{\mu}^* = 0$ and condition iii.\ is satisfied.
      Therefore, since $\vc{\lambda}^*$ and $\vc{\mu}^*$ both satisfy the KKT conditions stated above, $\vc{\lambda}^*$ is optimal for $(D_a)$.
  \end{enumerate}
\end{proof}


\end{document}

\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amsthm, amssymb, amsfonts, enumitem, graphicx}
\usepackage{fancyhdr}
\usepackage{tikz}
\usepackage{mathrsfs}
\usepackage{mathtools}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}

\theoremstyle{definition}
\newtheorem{problem}{Problem}
\renewcommand*{\proofname}{Solution}
\renewcommand{\theenumi}{\alph{enumi}}

\newcommand{\vc}[1]{\boldsymbol{#1}}

\newenvironment{amatrix}[1]{%
  \left[\begin{array}{@{}*{#1}{c}|c@{}}
}{%
  \end{array}\right]
}

\newcommand{\grstep}[2][\relax]{%
   \ensuremath{\mathrel{
       {\mathop{\longrightarrow}\limits^{#2\mathstrut}_{
                                     \begin{subarray}{l} #1 \end{subarray}}}}}}
\newcommand{\swap}{\leftrightarrow}
\newcommand{\tran}{\mathsf{T}}
\newtheorem{theorem}{Theorem}



\pagestyle{fancy}
\fancyhf{}
\rhead{Homework Assignment 6}
\lhead{Matthew Tiger}
\cfoot{\thepage}


\title{Homework Assignment 6}
\author{Matthew Tiger}


\begin{document}


\maketitle


% Problem 1
\begin{problem}
  \begin{enumerate}
    \item Where is the assumption ``$\vc{x}^*$ is regular'' essential in the proof of the results of section: Lagrange Multipliers?
    \item In the example on page 49 (Example 20.8 in \textit{An Introduction to Optimization}) explain in what way is $(P_0)$ equivalent to $(P_1)$.
    \item State the SOSC Theorem on p.\ 51 (Theorem 20.5 p.\ 474 in the book) for $\vc{x}^*$ a local maximizer.
  \end{enumerate}
\end{problem}

\begin{proof}
  \begin{enumerate}
    \item The assumption that $\vc{x}^*$ is regular is essential in  the proof of the Lagrange Multipliers Theorem
      in applying the results of Theorem 20.1, i.e.\ assuming that $\vc{y}\in T(\vc{x}^*)$ if and only if
      there exists a differentiable curve in $S$ passing through $\vc{x}^*$
      with derivative $\vc{y}$ at $\vc{x}^*$ for $\vc{x}^*$ a regular point.
    \item The two problems to consider are:
      \begin{align*}
        \begin{array}{ll}
          \begin{array}{rrl}
            (P_0) & \text{maximize} & \frac{\vc{x}^\tran Q \vc{x}}{\vc{x}^\tran P \vc{x}}\\
            & \text{subject to} & Q = Q^\tran \geq 0 \\
            & & P = P^\tran > 0.
          \end{array}
          &
          \begin{array}{rrl}
            (P_1) & \text{maximize} & \vc{x}^\tran Q \vc{x}\\
            & \text{subject to} & \vc{x}^\tran P \vc{x} = 1.
          \end{array}
        \end{array}
      \end{align*}
      Note that if $P$ is positive semi-definite and $Q$ is positive definite, then
      $\vc{x}^\tran Q \vc{x} \geq 0$ and $\vc{x}^\tran P \vc{x} > 0$ for every $\vc{x}$. Consequently
      \begin{align*}
        \frac{\vc{x}^\tran Q \vc{x}}{\vc{x}^\tran P \vc{x}} \geq 0
      \end{align*}
      for every $\vc{x}$. From problem $(P_0)$ we see that if $\vc{x}$ is a solution to the problem,
      then so is $t\vc{x}$ for any $t \neq 0$. Note that
      \begin{align*}
        \frac{(t\vc{x})^\tran Q (t\vc{x})}{(t\vc{x})^\tran P (t\vc{x})} = \frac{t^2 \vc{x}^\tran Q \vc{x}}{t^2 \vc{x}^\tran P \vc{x}} = \frac{\vc{x}^\tran Q \vc{x}}{\vc{x}^\tran P \vc{x}}
      \end{align*}
      showing that the above remark is true. Adding the additional constraint to problem $(P_0)$ that $\vc{x}^\tran P \vc{x} = 1$ removes the multiplicity of the solutions and transforms
      the original problem into problem $(P_1)$. To see this, if the constraint $\vc{x}^\tran P \vc{x} = 1$
      is satisfied then for any non-zero scalar multiple of $\vc{x}^*$ we have that
      \begin{align*}
        (t\vc{x})^\tran P (t\vc{x}) = t^2 \vc{x}^\tran P\vc{x} = \vc{x}^\tran P\vc{x}
      \end{align*}
      Since $\vc{x}^\tran P\vc{x} > 0$ we must have that $t=1$ removing the multiplicity of the solutions
      and the problems are equivalent.

    \item \begin{theorem}[\emph{\textbf{Second-Order Sufficient Conditions}}]
        Suppose that $f, \vc{h} \in \mathcal{C}^2$ with $f:\mathbb{R}^n \to \mathbb{R}$
        and $\vc{h}:\mathbb{R}^n \to \mathbb{R}^m$. Let $l(\vc{x},\vc{\lambda}) = f(\vc{x}) + \lambda_1 h_1(\vc{x}) + \lambda_2 h_2(\vc{x}) + \dots \lambda_m h_m(\vc{x})$
        be the Lagrangian function. Let
        \begin{align*}
          \vc{F}(\vc{x}) =
          \begin{bmatrix}
            \frac{\partial^2 f}{\partial x_1^2}(\vc{x}) & \frac{\partial^2 f}{\partial x_2 \partial x_1}(\vc{x}) & \dots & \frac{\partial^2 f}{\partial x_n \partial x_1}(\vc{x}) \\
            \frac{\partial^2 f}{\partial x_1 \partial x_2}(\vc{x}) & \frac{\partial^2 f}{\partial x_2^2}(\vc{x}) & \dots & \frac{\partial^2 f}{\partial x_n \partial x_2}(\vc{x}) \\
            \vdots & \vdots & \ddots & \vdots \\
            \frac{\partial^2 f}{\partial x_1 \partial x_n}(\vc{x}) & \frac{\partial^2 f}{\partial x_2 \partial x_n}(\vc{x}) & \dots & \frac{\partial^2 f}{\partial x_n^2}(\vc{x})
          \end{bmatrix}
        \end{align*}
        be the Hessian matrix of $f$ at $\vc{x}$ and
        \begin{align*}
          \vc{H_k}(\vc{x}) =
          \begin{bmatrix}
            \frac{\partial^2 h_k}{\partial x_1^2}(\vc{x}) & \frac{\partial^2 h_k}{\partial x_2 \partial x_1}(\vc{x}) & \dots & \frac{\partial^2 h_k}{\partial x_n \partial x_1}(\vc{x}) \\
            \frac{\partial^2 h_k}{\partial x_1 \partial x_2}(\vc{x}) & \frac{\partial^2 h_k}{\partial x_2^2}(\vc{x}) & \dots & \frac{\partial^2 h_k}{\partial x_n \partial x_2}(\vc{x}) \\
            \vdots & \vdots & \ddots & \vdots \\
            \frac{\partial^2 h_k}{\partial x_1 \partial x_n}(\vc{x}) & \frac{\partial^2 h_k}{\partial x_2 \partial x_n}(\vc{x}) & \dots & \frac{\partial^2 h_k}{\partial x_n^2}(\vc{x})
          \end{bmatrix}
        \end{align*}
        be the Hessian matrix of $h_k$ at $\vc{x}$ for $k=1,\dots,m$. Define
        \begin{align*}
          \vc{L}(\vc{x},\vc{\lambda}) = \vc{F}(\vc{x}) + \lambda_1 \vc{H_1}(\vc{x}) + \dots + \lambda_m \vc{H_m}(\vc{x})
        \end{align*}
        to be the Hessian Matrix of $l(\vc{x}, \vc{\lambda})$ with respect to $\vc{x}$.

        Suppose there exists a point $\vc{x}^* \in \mathbb{R}^n$ and $\vc{\lambda^*} \in \mathbb{R}^m$ such that
        \begin{itemize}
          \item $D f(\vc{x}^*) + \vc{\lambda}^{*\tran} D \vc{h}(\vc{x}^*) = \vc{0}^\tran$.
          \item For all $\vc{y}\in T(\vc{x}^*)$, $\vc{y}\neq \vc{0}$, we have that $\vc{y}^\tran \vc{L}(\vc{x}^*, \vc{\lambda}^{*})\vc{y} < 0$, i.e.\ $\vc{L}(\vc{x}^*, \vc{\lambda}^{*})$ is negative definite on $T(\vc{x}^*)$.
        \end{itemize}
        Then $\vc{x}^*$ is a strict local maximizer of $f$ subject to $\vc{h}(\vc{x}) = \vc{0}.$
      \end{theorem}
  \end{enumerate}
\end{proof}
\newpage


% Problem 2
\begin{problem}
  Find local extremizers for the following optimization problem:
  \begin{align*}
    \begin{array}{rl}
      \text{maximize} & x_1x_2 \\
      \text{subject to} & x_1^2 + 4x_2^2 = 1.
    \end{array}
  \end{align*}
\end{problem}

\begin{proof}
  Lagrange's Theorem prescribes how to find the possible local extremizers for the optimization problem. Let $f(\vc{x}) = x_1x_2$
  and $h(\vc{x}) = x_1^2 + 4x_2^2 - 1$.
  Note that we then have that
  \begin{align*}
    \triangledown f(\vc{x})^\tran =
    \begin{bmatrix}
      x_2 &
      x_1
    \end{bmatrix}
    \quad \text{and} \quad
    \triangledown h(\vc{x})^\tran =
    \begin{bmatrix}
      2x_1 &
      8x_2
    \end{bmatrix}.
  \end{align*}
  Since for every feasible $\vc{x}$, the Jacobian of $\vc{h}$ is of rank 1, i.e.\ of full rank,
  every feasible point is a regular point. Using the Lagrange condition
  $D f(\vc{x}^*) + \vc{\lambda}^{*\tran} D \vc{h}(\vc{x}^*) = \vc{0}^\tran$, we formulate
  the system of equations
  \begin{align*}
    D f(\vc{x}^*) + \vc{\lambda}^{*\tran} D \vc{h}(\vc{x}^*) =
    \begin{bmatrix}x_2 + 2\lambda x_1 & x_1 + 8\lambda x_2\end{bmatrix} =
    \begin{bmatrix}0 & 0\end{bmatrix} = \vc{0}^\tran
  \end{align*}
  for $\lambda \in \mathbb{R}.$
  Thus, an extremizer of the optimization problem must satisfy the following system of equations
  \begin{align*}
    x_2 + 2\lambda x_1 &= 0 \\
    x_1 + 8 \lambda x_2 &= 0 \\
    x_1^2 + 4x_2^2  &=1.
  \end{align*}

  Solving the above system leads to four distinct solutions.
  Thus, the possible extremizers to the optimization problem are
  \begin{align*}
    \vc{x}^{(1)} = \begin{bmatrix}1 / \sqrt{2} \\ 1 / (2\sqrt{2})\end{bmatrix},\
    \vc{x}^{(2)} = \begin{bmatrix}1 / \sqrt{2} \\ -1 / (2\sqrt{2})\end{bmatrix},\
    \vc{x}^{(3)} = \begin{bmatrix}-1 / \sqrt{2} \\ 1 / (2\sqrt{2})\end{bmatrix},\
    \vc{x}^{(4)} = \begin{bmatrix}-1 / \sqrt{2} \\ -1 / (2\sqrt{2})\end{bmatrix}
  \end{align*}
  where the Lagrange multiplier for $\vc{x}^{(1)}$ and $\vc{x}^{(4)}$ is $\lambda = -1/4$
  while the Lagrange multiplier for $\vc{x}^{(2)}$ and $\vc{x}^{(3)}$ is $\lambda = 1/4$.
  Since $f(\vc{x}^{(1)}) = f(\vc{x}^{(4)}) = 1/4$ and $f(\vc{x}^{(2)}) = f(\vc{x}^{(3)}) = -1/4$, we have that the possible
  maximizers of the problem are located at $\vc{x}^{(1)}$ and $\vc{x}^{(4)}$ while the
  possible minimizers of the problem are located at $\vc{x}^{(2)}$ and $\vc{x}^{(3)}$.

  The Second Order Sufficient Conditions prescribe how to check if these extremizers
  are strict local extremizers. We have already shown that these extremizers satisfy the first condition, i.e.\ the
  Lagrange condition. We must now show that for $\vc{y} \neq \vc{0} \in T(\vc{x}^{(i)})$ for $i=1,\dots,4$
  we have that
  \begin{align}\label{second}
    \vc{y}^\tran (D^2 f(\vc{x}^{(i)}) + \vc{\lambda}^{(i)\tran} D^2 \vc{h}(\vc{x}^{(i)}))\vc{y} &= \vc{y}^\tran\left(\begin{bmatrix}0 & 1 \\ 1 & 0\end{bmatrix} + \vc{\lambda}^{(i)\tran}\begin{bmatrix}2 & 0 \\ 0 &8\end{bmatrix}\right)\vc{y} > (<) 0
  \end{align}
  for $\vc{x}^{(i)}$ to be a strict local minimum (maximum).

  Note that if $\vc{y} \neq \vc{0} \in T(\vc{x})$ then
  \begin{align*}
    D\vc{h}(\vc{x})\vc{y} = \begin{bmatrix}2x_1 & 8x_2\end{bmatrix}\begin{bmatrix}y_1 \\ y_2\end{bmatrix} = 2(x_1y_1 + 4x_2y_2) = 0.
  \end{align*}
  We wish to see if $\vc{x}^{(1)}$ and $\vc{x}^{(4)}$ are strict local maximizers. Start with $\vc{x}^{(1)} = \begin{bmatrix}1/\sqrt{2} & 1/(2\sqrt{2})\end{bmatrix}^\tran$.
  The Lagrange multiplier associated to this solution is $\vc{\lambda}^{(1)} = -1/4$. Then $\vc{y} \neq \vc{0} \in T(\vc{x}^{(1)}) = \{\vc{y}\ |\  \vc{y} = [y_1 \ -y_1/2]\}$. From this we see that \eqref{second} becomes
  \begin{align*}
    \begin{bmatrix}y_1 \\ -y_1/2\end{bmatrix}^\tran\left(\begin{bmatrix}0 & 1 \\ 1 & 0\end{bmatrix} - 1/4\begin{bmatrix}2 & 0 \\ 0 &8\end{bmatrix}\right)\begin{bmatrix}y_1 \\ -y_1/2\end{bmatrix} &=
    \begin{bmatrix}y_1 \\ -y_1/2\end{bmatrix}^\tran\begin{bmatrix}-1/2 & 1 \\ 1 & -2\end{bmatrix}\begin{bmatrix}y_1 \\ -y_1/2\end{bmatrix} = -2y_1^2 < 0
  \end{align*}
  showing indeed that $\vc{x}^{(1)}$ is a strict local maximizer.

  Now let's check $\vc{x}^{(4)} = \begin{bmatrix}-1/\sqrt{2} & -1/(2\sqrt{2})\end{bmatrix}^\tran$.
  The Lagrange multiplier associated to this solution is $\vc{\lambda}^{(4)} = -1/4$. Then $\vc{y} \neq \vc{0} \in T(\vc{x}^{(4)}) = \{\vc{y}\ |\  \vc{y} = [y_1 \ -y_1/2]\}$. From this we see that \eqref{second} becomes
  \begin{align*}
    \begin{bmatrix}y_1 \\ -y_1/2\end{bmatrix}^\tran\left(\begin{bmatrix}0 & 1 \\ 1 & 0\end{bmatrix} - 1/4\begin{bmatrix}2 & 0 \\ 0 &8\end{bmatrix}\right)\begin{bmatrix}y_1 \\ -y_1/2\end{bmatrix} &=
    \begin{bmatrix}y_1 \\ -y_1/2\end{bmatrix}^\tran\begin{bmatrix}-1/2 & 1 \\ 1 & -2\end{bmatrix}\begin{bmatrix}y_1 \\ -y_1/2\end{bmatrix} = -2y_1^2 < 0
  \end{align*}
  showing indeed that $\vc{x}^{(4)}$ is a strict local maximizer.

  We wish to now see if $\vc{x}^{(2)}$ and $\vc{x}^{(3)}$ are strict local minimizers. Start with $\vc{x}^{(2)} = \begin{bmatrix}1/\sqrt{2} & -1/(2\sqrt{2})\end{bmatrix}^\tran$.
  The Lagrange multiplier associated to this solution is $\vc{\lambda}^{(2)} = 1/4$. Then $\vc{y} \neq \vc{0} \in T(\vc{x}^{(2)}) = \{\vc{y}\ |\  \vc{y} = [y_1 \ y_1/2]\}$. From this we see that \eqref{second} becomes
  \begin{align*}
    \begin{bmatrix}y_1 \\ y_1/2\end{bmatrix}^\tran\left(\begin{bmatrix}0 & 1 \\ 1 & 0\end{bmatrix} + 1/4\begin{bmatrix}2 & 0 \\ 0 &8\end{bmatrix}\right)\begin{bmatrix}y_1 \\ y_1/2\end{bmatrix} &=
    \begin{bmatrix}y_1 \\ y_1/2\end{bmatrix}^\tran\begin{bmatrix}1/2 & 1 \\ 1 & 2\end{bmatrix}\begin{bmatrix}y_1 \\ y_1/2\end{bmatrix} = 2y_1^2 > 0
  \end{align*}
  showing indeed that $\vc{x}^{(2)}$ is a strict local minimizer.

  Now let's check $\vc{x}^{(3)} = \begin{bmatrix}-1/\sqrt{2} & 1/(2\sqrt{2})\end{bmatrix}^\tran$.
  The Lagrange multiplier associated to this solution is $\vc{\lambda}^{(3)} = 1/4$. Then $\vc{y} \neq \vc{0} \in T(\vc{x}^{(3)}) = \{\vc{y}\ |\  \vc{y} = [y_1 \ y_1/2]\}$. From this we see that \eqref{second} becomes
  \begin{align*}
    \begin{bmatrix}y_1 \\ y_1/2\end{bmatrix}^\tran\left(\begin{bmatrix}0 & 1 \\ 1 & 0\end{bmatrix} + 1/4\begin{bmatrix}2 & 0 \\ 0 &8\end{bmatrix}\right)\begin{bmatrix}y_1 \\ y_1/2\end{bmatrix} &=
    \begin{bmatrix}y_1 \\ y_1/2\end{bmatrix}^\tran\begin{bmatrix}1/2 & 1 \\ 1 & 2\end{bmatrix}\begin{bmatrix}y_1 \\ y_1/2\end{bmatrix} = 2y_1^2 > 0
  \end{align*}
  showing indeed that $\vc{x}^{(3)}$ is a strict local minimizer.
\end{proof}
\newpage


% Problem 3
\begin{problem}
  Consider the problem
  \begin{align*}
    \begin{array}{rll}
      \text{minimize} & 2x_1 + 3x_2 - 4, & x_1, x_2 \in \mathbb{R} \\
      \text{subject to} & x_1 x_2 = 6.&
    \end{array}
  \end{align*}

  \begin{enumerate}
    \item Use Lagrange's theorem to find all possible local minimizers and maximizers.
    \item Use the second-order sufficient conditions to specify which points are strict
      local minimizers and which are strict local maximizers.
    \item Are the points in part b global minimizers or maximizers? Explain.
  \end{enumerate}
\end{problem}

\begin{proof}
  \begin{enumerate}
    \item We use Lagrange's theorem to find the possible extremizers of this optimization problem.
      To start, say $f(\vc{x}) = 2x_1 + 3x_2 - 4$ and $h(\vc{x}) = x_1 x_2 - 6$.
      Then
      \begin{align*}
        \triangledown f(\vc{x})^\tran = \begin{bmatrix}2 & 3\end{bmatrix}
        \quad \text{and} \quad
        \triangledown \vc{h}(\vc{x})^\tran = \begin{bmatrix}x_2 & x_1\end{bmatrix}.
      \end{align*}
      Since the Jacobian of $\vc{h}$ is of full rank only if $x_1 \neq 0$ and $x_2 \neq 0$, we have that for every
      feasible point the Jacobian of $\vc{h}$ is of full rank, i.e.\ every feasible point is regular.
      The Lagrange condition states that the possible extremizers $\vc{x}^{*}$ satisfy $D f(\vc{x}^*) + \vc{\lambda}^{*\tran} D \vc{h}(\vc{x}^*) = \vc{0}^\tran$
      or
      \begin{align*}
        \begin{bmatrix}2 + \lambda x_2 & 3 + \lambda x_1\end{bmatrix} = \begin{bmatrix}0 & 0\end{bmatrix}.
      \end{align*}
      The Lagrange condition in conjunction with the condition that $\vc{h}(\vc{x}) = \vc{0}$ yields the following
      system of equations:
      \begin{align*}
        2 + \lambda x_2 &= 0 \\
        3 + \lambda x_1 &= 0 \\
        x_1 x_2 &= 6.
      \end{align*}
      From the first two equations, we see that $x_1 = -3/\lambda$ and $x_2 = -2/\lambda$. Substituting into the
      third equation yields that $6 = 6/\lambda^2$ or that $\lambda=\pm 1$. Thus, the possible extremizers of
      the optimization problem are
      \begin{align}\label{extremizers}
        \vc{x}^{(1)} = \begin{bmatrix}-3 \\ -2\end{bmatrix} ,\
        \vc{x}^{(2)} = \begin{bmatrix}3 \\ 2\end{bmatrix}.
      \end{align}
    \item The second-order sufficient conditions allow us to specify whether
      the possible extremizers \eqref{extremizers} are strict local minimizers or maximizers.
      We have already shown that these solutions satisfy the first condition, i.e.\ these solutions satisfy the Lagrange condition.
      We need to show that for the solutions $\vc{x}^{(i)}$, $i=1,2$, in \eqref{extremizers}, it is true that
      for all $\vc{y}  \neq \vc{0} \in T(\vc{x}^{(i)})$, $\vc{y}\neq \vc{0}$, we have
      \begin{align*}
        \vc{y}^\tran \left(
          \begin{bmatrix}
            \frac{\partial^2 f}{\partial x_1^2}(\vc{x}^{(i)}) & \frac{\partial^2 f}{\partial x_2\partial x_1}(\vc{x}^{(i)}) \\
            \frac{\partial^2 f}{\partial x_1\partial x_2}(\vc{x}^{(i)}) & \frac{\partial^2 f}{\partial x_2^2}(\vc{x}^{(i)}) \\
          \end{bmatrix} + \lambda
          \begin{bmatrix}
            \frac{\partial^2 \vc{h}}{\partial x_1^2}(\vc{x}^{(i)}) & \frac{\partial^2 \vc{h}}{\partial x_2\partial x_1}(\vc{x}^{(i)}) \\
            \frac{\partial^2 \vc{h}}{\partial x_1\partial x_2}(\vc{x}^{(i)}) & \frac{\partial^2 \vc{h}}{\partial x_2^2}(\vc{x}^{(i)}) \\
          \end{bmatrix}
        \right) \vc{y} > (<)\ 0
      \end{align*}
      to show that $\vc{x}^{(i)}$ is a strict local minimizer (maximizer).

      Take the solution $\vc{x}^{(1)} = \begin{bmatrix}-3 & -2\end{bmatrix}^\tran$.
      The Lagrange multiplier associated to this solution was $\lambda=1$. We must
      now check that for any $\vc{y} = \begin{bmatrix}y_1 & y_2\end{bmatrix}^\tran \neq \vc{0} \in T(\vc{x}^{(1)})$, we have that
      \begin{align*}
        \vc{y}^\tran \left(\begin{bmatrix}0 & 0 \\ 0 & 0\end{bmatrix} + 1 \begin{bmatrix}0 & 1 \\ 1 & 0\end{bmatrix}\right) \vc{y} =
         \vc{y}^\tran \begin{bmatrix}0 & 1 \\ 1 & 0\end{bmatrix} \vc{y} = 2y_1 y_2 > (<)\ 0
      \end{align*}
      for $\vc{x}^{(1)}$ to be a strict local minimizer (maximizer).
      Note that $\vc{y}\in T(\vc{x}^{(1)})$ if
      \begin{align*}
        D \vc{h}(\vc{x}^{(1)})\vc{y} = \triangledown \vc{h}(\vc{x}^{(1)})^\tran \vc{y} = \begin{bmatrix}-2 -3\end{bmatrix}\begin{bmatrix}y_1 \\ y_2\end{bmatrix} = 0
      \end{align*}
      From this equation we see that for $\vc{y}\in T(\vc{x}^{(1)})$, we have that $y_2 = (-2/3 )y_1$ and that $2y_1 y_2 = (-4/3)y_1^2< 0$.
      Thus, this solution is a strict local maximizer.

      Now take the solution $\vc{x}^{(2)} = \begin{bmatrix}3 & 2\end{bmatrix}^\tran$.
      The Lagrange multiplier associated to this solution was $\lambda=-1$. We must
      now check that for any $\vc{y} = \begin{bmatrix}y_1 & y_2\end{bmatrix}^\tran \neq \vc{0}\in T(\vc{x}^{(2)})$, we have that
      \begin{align*}
        \vc{y}^\tran \left(\begin{bmatrix}0 & 0 \\ 0 & 0\end{bmatrix} -1 \begin{bmatrix}0 & 1 \\ 1 & 0\end{bmatrix}\right) \vc{y} =
        \vc{y}^\tran \begin{bmatrix}0 & -1 \\ -1 & 0\end{bmatrix} \vc{y} = -2y_1 y_2 > (<)\ 0.
      \end{align*}
      for $\vc{x}^{(1)}$ to be a strict local minimizer (maximizer).
      Note that $\vc{y}\in T(\vc{x}^{(2)})$ if
      \begin{align*}
        D \vc{h}(\vc{x}^{(2)})\vc{y} = \triangledown \vc{h}(\vc{x}^{(2)})^\tran \vc{y} = \begin{bmatrix}2 & 3\end{bmatrix}\begin{bmatrix}y_1 \\ y_2\end{bmatrix} = 0
      \end{align*}
      From this equation we see that for $\vc{y}\in T(\vc{x}^{(2)})$, we have that $y_2 = (-2/3 )y_1$ and that $-2y_1 y_2 = (4/3)y_1^2 > 0$.
      Thus, this solution is a strict local minimizer.

    \item The strict local minimizer $\vc{x}^{(2)}$ is not a global minimizer and the strict local maximizer $\vc{x}^{(1)}$ is not a global maximizer.
      To see why this is true, take note that if $\vc{x}^{*}$ is a global minimizer (maximizer),
      then $f(\vc{x}^{*}) \leq (\geq)\ f(\vc{x})$ for all $\vc{x}$ such that $h(\vc{x})=\vc{0}$, i.e. $\vc{x}$ is feasible.
      Also take note that $f(\vc{x}^{(2)}) = 8$ and $f(\vc{x}^{(1)}) = -16$.

      If $\vc{x}^{(2)}$ were a global minimizer, then $f(\vc{x}^{(2)})=8 \leq  f(\vc{x})$
      for all feasible $\vc{x}$. However, $\vc{x}^{(1)}$ is feasible, yet $f(\vc{x}^{(2)}) \nleq -16 = f(\vc{x}^{(1)})$
      showing that $\vc{x}^{(2)}$ cannot be a global minimizer.

      In a very similar fashion we argue that $\vc{x}^{(1)}$ is not a global maximizer.
      For if $\vc{x}^{(1)}$ were a global maximizer, then $f(\vc{x}^{(1)})=-16 \geq  f(\vc{x})$
      for all feasible $\vc{x}$. However, $\vc{x}^{(2)}$ is feasible, yet $f(\vc{x}^{(1)}) \ngeq 8 = f(\vc{x}^{(2)})$
      showing that $\vc{x}^{(1)}$ cannot be a global maximizer.
  \end{enumerate}
\end{proof}
\newpage


% Problem 4
\begin{problem}
  Consider the problem of minimizing a general quadratic function subject to a linear constraint:
  \begin{align*}
    \begin{array}{rl}
      \text{minimize} & \frac{1}{2} \vc{x}^\tran Q \vc{x} - \vc{c}^\tran \vc{x} + d\\
      \text{subject to} & A\vc{x} = \vc{b},
    \end{array}
  \end{align*}
  where $Q=Q^\tran > 0$, $A \in\mathbb{R}^{m \times n}$ with $m<n$, $\text{rank} A = m$ and $d$ a constant.
  Derive a closed form solution to the problem.
\end{problem}

\begin{proof}
  From the above problem, we wish to minimize $f(\vc{x}) = \frac{1}{2} \vc{x}^\tran Q \vc{x} - \vc{c}^\tran \vc{x} + d$
  subject to $\vc{h}(\vc{x}) = \vc{b} - A\vc{x}= \vc{0}$. We need to identify the possible
  minimizers of this optimization problem.

  In order to proceed, we form the Lagrange function as
  \begin{align*}
    l(\vc{x}, \vc{\lambda}) &= f(\vc{x}) + \vc{\lambda}^\tran \vc{h}(\vc{x}) \\ &= \frac{1}{2} \vc{x}^\tran Q \vc{x} - \vc{c}^\tran \vc{x} + d + \vc{\lambda}^\tran(\vc{b} - A\vc{x}).
  \end{align*}
  By Lagrange's Theorem, the possible minimizers satisfy
  \begin{align}\label{FONC}
    D_x l(\vc{x}^{*}, \vc{\lambda}^{*}) = D_x f(\vc{x}^{*}) + \vc{\lambda}^{*}D_x \vc{h}(\vc{x}^{*}) = \vc{0}^\tran.
  \end{align}
  Note that
  \begin{align*}
    D_x \left(\frac{1}{2} \vc{x}^\tran Q \vc{x}\right) &= \frac{1}{2} D_x \left(\sum_{i=1}^n \sum_{j=1}^n q_{ij} x_i x_j\right) \\
    &= \frac{1}{2} \begin{bmatrix}\sum_{i}\sum_{j} q_{ij} \frac{\partial}{\partial x_1}\left[x_ix_j\right], & \dots, & \sum_{i}\sum_{j} q_{ij} \frac{\partial}{\partial x_n}\left[x_ix_j\right]\end{bmatrix} \\
    &= \frac{1}{2} \begin{bmatrix}\sum_{i}\sum_{j} q_{ij} \left[\frac{\partial x_i}{\partial x_1}x_j + \frac{\partial x_j}{\partial x_1}x_i\right], & \dots, & \sum_{i}\sum_{j} q_{ij}\left[\frac{\partial x_i}{\partial x_n}x_j + \frac{\partial x_j}{\partial x_n}x_i\right] \end{bmatrix} \\
    &= \begin{bmatrix}x_1 \sum_{i}\sum_{j} q_{ij}, & \dots, & x_n\sum_{i}\sum_{j} q_{ij} \end{bmatrix} \\
    &= \vc{x}^\tran Q
  \end{align*}
  since $\frac{\partial x_i}{\partial x_k} = 0$ if $i\neq k$. Therefore, we have that the FONC \eqref{FONC} become
  \begin{align*}
    D_x l(\vc{x}^{*}, \vc{\lambda}^{*}) = \vc{x}^{*\tran} Q - \left(\vc{\lambda}^{*\tran} A + \vc{c}^\tran\right) = \vc{0}^\tran
  \end{align*}
  and by Lagrange's Theorem, the only possible minimizer is
  \begin{align}\label{minimizer_1}
    \vc{x}^{*} = Q^{-1}A^\tran \vc{\lambda}^{*} + Q^{-1}\vc{c}.
  \end{align}
  In order to find a closed-form solution to the problem we must identify $\vc{\lambda}^{*}$. Multiplying
  \eqref{minimizer} by $A$ and using the condition that $A\vc{x}^{*} = b$ we have that
  \begin{align*}
    \vc{b} = AQ^{-1}A^\tran \vc{\lambda}^{*} + AQ^{-1}\vc{c}.
  \end{align*}
  Since $Q$ is positive definite and $A$ is of full rank, $AQ^{-1}A^\tran$ is invertible and
  \begin{align*}
    \vc{\lambda}^{*} = (AQ^{-1}A^\tran)^{-1}(\vc{b} - AQ^{-1}\vc{c}).
  \end{align*}
  Therefore, from \eqref{minimizer_1}, the possible minimizer to the optimization problem in closed-form is
  \begin{align}\label{closed}
    \vc{x}^{*} = Q^{-1}A^\tran(AQ^{-1}A^\tran)^{-1}(\vc{b} - AQ^{-1}\vc{c}) + Q^{-1}\vc{c}.
  \end{align}

  In order to verify that the minimizer \eqref{closed} is in fact a minimizer,
  we must check that the Hessian matrix of the Lagrange function is positive definite on the tangent space of $\vc{x}^{*}$.
  We readily see that the Hessian matrix of the Lagrange function is
  \begin{align*}
    D^2_x l(\vc{x}^{*}, \vc{\lambda}^{*}) = D_x(\vc{x}^{*\tran} Q - \left(\vc{\lambda}^{*\tran} A + \vc{c}^\tran\right)) = Q.
  \end{align*}
  Since $Q$ is positive definite, we have that the Hessian matrix of the Lagrange function is most certainly
  positive definite on the tangent space of $\vc{x}^{*}$ showing that the minimizer \eqref{closed} is indeed a minimizer by the Second Order Sufficient Conditions.
\end{proof}
\newpage


% Problem 5
\begin{problem}
  Consider the discrete-time linear system $x_k = 2 x_{k-1} + u_k$, $k \geq 1$, with
  $x_0 = 1$. Find the values of the control inputs $u_1$ and $u_2$ to minimize
  $$x_2^2 + \frac{1}{2}u_1^2+ \frac{1}{3}u_2^2.$$
\end{problem}

\begin{proof}
  Note that it can easily be proven that the recurrence relation
  $x_k = 2 x_{k-1} + u_k$, $k \geq 1$ has the closed form solution
  \begin{align*}
    x_n = 2^n x_0 + \sum_{k=1}^n 2^{n-k} u_k \quad \text{for $n\geq 1$}.
  \end{align*}
  Thus, our control inputs are subject to the constraints
  \begin{align*}
    x_1 &= 2x_0 + u_1 \\
    x_1 &= 4x_0 + 2u_1 + u_2
  \end{align*}
  which can be reformulated as
  \begin{align}\label{con}
    2x_0 &= x_1 - u_1 \notag \\
    4x_0 &= x_2 - 2u_1 - u_2.
  \end{align}
  Forming the vector of decision variables as $\vc{z}^\tran = \begin{bmatrix}x_1 & x_2 & z_1 & z_2\end{bmatrix}$,
  we can write the constraints \eqref{con} as
  \begin{align*}
    \begin{bmatrix}
      1 & 0 & -1 & 0 \\
      0 & 1 & -2 & -1 \\
    \end{bmatrix}
    \vc{z}
    =
    \begin{bmatrix}
      2 x_0 \\
      4 x_0
    \end{bmatrix}.
  \end{align*}
  Using the same decision variable vector $\vc{z}$, we can write the object function
  as
  \begin{align*}
    x_2^2 + \frac{1}{2}u_1^2+ \frac{1}{3}u_2^2
    = \frac{1}{2}\vc{z}^\tran
    \begin{bmatrix}
      0 & 0 & 0 & 0 \\
      0 & 2 & 0 & 0 \\
      0 & 0 & 1 & 0 \\
      0 & 0 & 0 & \frac{2}{3} \\
   \end{bmatrix}\vc{z}.
  \end{align*}
  Combining these facts, we can write the original problem as the following
  quadratic programming problem
  \begin{align}\label{quad_prob}
    \begin{array}{rl}
      \text{minimize} & \frac{1}{2} \vc{z}^\tran Q \vc{z}\\
      \text{subject to} & A \vc{z} = \vc{b}\\
    \end{array}
  \end{align}
  where the objective is $f(\vc{z}) = \frac{1}{2} \vc{z}^\tran Q \vc{z}$, the constraint is $\vc{h}(\vc{x}) = \vc{b}-A \vc{z} $, and
  \begin{align}\label{defs}
    \vc{z} = \begin{bmatrix}x_1 \\ x_2 \\ u_1 \\ u_2\end{bmatrix}, \quad
    Q =
    \begin{bmatrix}
      0 & 0 & 0 & 0 \\
      0 & 2 & 0 & 0 \\
      0 & 0 & 1 & 0 \\
      0 & 0 & 0 & \frac{2}{3} \\
   \end{bmatrix}, \quad
   A =
   \begin{bmatrix}
      1 & 0 & -1 & 0 \\
      0 & 1 & -2 & -1 \\
   \end{bmatrix}, \quad
   \vc{b} =
   \begin{bmatrix}
     2x_0 \\
     4x_0 \\
   \end{bmatrix}.
  \end{align}

  Note that for this problem the matrix $Q$ is singular. The Lagrangian associated
  to this problem is
  \begin{align*}
    l(\vc{z}, \vc{\lambda}) = \frac{1}{2}\vc{z}^\tran Q \vc{z} + \vc{\lambda}^\tran(\vc{b} - A \vc{z}).
  \end{align*}
  The Lagrange condition shows that
  \begin{align*}
    D_z l(\vc{z}, \vc{\lambda}) = \vc{z}^\tran Q - \vc{\lambda}^\tran A = \vc{0}^\tran \implies
    D_z l(\vc{z}, \vc{\lambda}) = Q\vc{z}  = A^\tran \vc{\lambda}.
  \end{align*}
  Using our definitions of the above matrices we see that for $\vc{\lambda}^\tran = \begin{bmatrix}\lambda_1 & \lambda_2\end{bmatrix}$
  that
  \begin{align*}
    0 = \lambda_2, \quad
    x_2 = \frac{\lambda_2}{2}, \quad
    u_1 = -\lambda_1-2\lambda_2, \quad
    u_2 = -\frac{3}{2}\lambda_2
  \end{align*}
  from which we see that the possible minimizer of the problem is
  \begin{align*}
    \vc{z}^{*\tran} =
    \begin{bmatrix}
      x_1 &
      0 &
      -\lambda_1 &
      0
    \end{bmatrix}.
  \end{align*}
  Using the fact that $A\vc{z}^{*} = \vc{b}$ we see that
  \begin{align*}
    A\vc{z}^{*}=
       \begin{bmatrix}
      1 & 0 & -1 & 0 \\
      0 & 1 & -2 & -1 \\
   \end{bmatrix}
    \begin{bmatrix}
      x_1 \\
      0 \\
      -\lambda_1 \\
      0\\
    \end{bmatrix}
    =\begin{bmatrix}
      x_1 + \lambda_1 \\
      2\lambda_1
    \end{bmatrix}
    =\begin{bmatrix}
     2x_0 \\
     4x_0 \\
   \end{bmatrix} = \vc{b}.
  \end{align*}
  Thus, $\lambda_1 = 2x_0$ and $x_1 = 0$. Therefore, the possible minimizer is
  \begin{align}\label{minimizer}
    \vc{z}^{*\tran} =
    \begin{bmatrix}
      0 &
      0 &
      -2x_0 &
      0
    \end{bmatrix}.
  \end{align}

  In order to show that this is in fact a strict local minimizer, we must show that
  the Hessian matrix of the Lagrange function,
  $D^2_z l(\vc{z}^{*},\vc{\lambda}^{*}) = Q$, satisfies
  \begin{align*}
    \text{For all $\vc{y} \in T(\vc{z}^{*})$, $\vc{y}\neq\vc{0}$ we have, }  \vc{y}^\tran Q \vc{y} > 0.
  \end{align*}
  Note that $y\in T(\vc{z}^{*})$ where $T(\vc{z}^{*}) = \{\vc{y}\ |\ D\vc{h}(\vc{z}^{*})\vc{y}=-A\vc{y}=\vc{0}\}$.
  This translates into the null space of the matrix $A$. Thus, we must have that if $y\in T(\vc{z}^{*})$, then
  \begin{align*}
    \vc{y}^\tran =  \begin{bmatrix}y_1 & y_2 & y_1 & -2y_1+y_2\end{bmatrix}.
  \end{align*}
  Using this expression we readily see that for $\vc{y} \neq \vc{0}\in T(\vc{z}^{*})$,
  \begin{align*}
    \vc{y}^\tran Q \vc{y} = y_1^2 + 2 y_2^2 + \frac{2}{3} (-2 y_1 + y_2)^2 > 0
  \end{align*}
  since $y_1 \neq 0$, $y_2 \neq 0$, and $-2 y_1 + y_2\neq 0$ and the SOSCs are satisfied.
  Thus, the minimizer \eqref{minimizer} is in fact a strict local minimizer and
  the values of the control input to minimize
  $$x_2^2 + \frac{1}{2}u_1^2+ \frac{1}{3}u_2^2$$ subject to   $x_k = 2 x_{k-1} + u_k$, $k \geq 1$, with
  $x_0 = 1$ are
  \begin{align*}
    u_1 = -2x_0 = -2 \quad \text{and} \quad u_2 = 0.
  \end{align*}
\end{proof}


\end{document}

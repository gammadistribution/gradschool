\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amsthm, amssymb, amsfonts, enumitem, graphicx}
\usepackage{fancyhdr}
\usepackage{tikz}
\usepackage{mathrsfs}
\usepackage{mathtools}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}

\theoremstyle{definition}
\newtheorem{problem}{Problem}
\renewcommand*{\proofname}{Solution}
\renewcommand{\theenumi}{\alph{enumi}}

\newcommand{\vc}[1]{\boldsymbol{#1}}

\newenvironment{amatrix}[1]{%
  \left[\begin{array}{@{}*{#1}{c}|c@{}}
}{%
  \end{array}\right]
}

\newcommand{\grstep}[2][\relax]{%
   \ensuremath{\mathrel{
       {\mathop{\longrightarrow}\limits^{#2\mathstrut}_{
                                     \begin{subarray}{l} #1 \end{subarray}}}}}}
\newcommand{\swap}{\leftrightarrow}
\newcommand{\tran}{\mathsf{T}}
\newtheorem{theorem}{Theorem}



\pagestyle{fancy}
\fancyhf{}
\rhead{Homework Assignment 6}
\lhead{Matthew Tiger}
\cfoot{\thepage}


\title{Homework Assignment 6}
\author{Matthew Tiger}


\begin{document}


\maketitle


% Problem 1
\begin{problem}
  \begin{enumerate}
    \item Where is the assumption ``$\vc{x}^*$ is regular'' essential in the proof of the results of section: Lagrange Multipliers?
    \item In the example on page 49 (Example 20.8 in \textit{An Introduction to Optimization}) explain in what way is $(P_0)$ equivalent to $(P_1)$.
    \item State the SOSC Theorem on p.\ 51 (Theorem 20.5 p.\ 474 in the book) for $\vc{x}^*$ a local maximizer.
  \end{enumerate}
\end{problem}

\begin{proof}
  \begin{enumerate}
    \item The assumption that $\vc{x}^*$ is regular is essential in  the proof of the Lagrange Multipliers Theorem
      in applying the results of Theorem 20.1, i.e.\ assuming that $\vc{y}\in T(\vc{x}^*)$ if and only if
      there exists a differentiable curve in $S$ passing through $\vc{x}^*$
      with derivative $\vc{y}$ at $\vc{x}^*$.
    \item The two problems to consider are:
      \begin{align*}
        \begin{array}{ll}
          \begin{array}{rrl}
            (P_0) & \text{maximize} & \frac{\vc{x}^\tran Q \vc{x}}{\vc{x}^\tran P \vc{x}}\\
            & \text{subject to} & Q = Q^\tran \geq 0 \\
            & & P = P^\tran > 0.
          \end{array}
          &
          \begin{array}{rrl}
            (P_1) & \text{maximize} & \vc{x}^\tran Q \vc{x}\\
            & \text{subject to} & \vc{x}^\tran P \vc{x} = 1.
          \end{array}
        \end{array}
      \end{align*}
      Note that if $P$ is positive semi-definite and $Q$ is positive definite, then
      $\vc{x}^\tran Q \vc{x} \geq 0$ and $\vc{x}^\tran P \vc{x} > 0$ for every $\vc{x}$. Consequently
      \begin{align*}
        \frac{\vc{x}^\tran Q \vc{x}}{\vc{x}^\tran P \vc{x}} \geq 0
      \end{align*}
      for every $\vc{x}$. From problem $(P_0)$ we see that if $\vc{x}$ is a solution to the problem,
      then so is $t\vc{x}$ for any $t \neq 0$. Note that
      \begin{align*}
        \frac{(t\vc{x})^\tran Q (t\vc{x})}{(t\vc{x})^\tran P (t\vc{x})} = \frac{t^2 \vc{x}^\tran Q \vc{x}}{t^2 \vc{x}^\tran P \vc{x}} = \frac{\vc{x}^\tran Q \vc{x}}{\vc{x}^\tran P \vc{x}}
      \end{align*}
      showing that the above remark is true. Adding the additional constraint to problem $(P_0)$ that $\vc{x}^\tran P \vc{x} = 1$ removes the multiplicity of the solutions and transforms
      the original problem into problem $(P_1)$. To see this, if the constraint $\vc{x}^\tran P \vc{x} = 1$
      is satisfied then for any non-zero scalar multiple of $\vc{x}^*$ we have that
      \begin{align*}
        (t\vc{x})^\tran P (t\vc{x}) = t^2 \vc{x}^\tran P\vc{x} = \vc{x}^\tran P\vc{x}
      \end{align*}
      Since $\vc{x}^\tran P\vc{x} > 0$ we must have that $t=1$ removing the multiplicity of the solutions
      and the problems are equivalent.

    \item \begin{theorem}[\emph{\textbf{Second-Order Sufficient Conditions}}]
        Suppose that $f, \vc{h} \in \mathcal{C}^2$ with $f:\mathbb{R}^n \to \mathbb{R}$
        and $\vc{h}:\mathbb{R}^n \to \mathbb{R}^m$. Let $l(\vc{x},\vc{\lambda}) = f(\vc{x}) + \lambda_1 h_1(\vc{x}) + \lambda_2 h_2(\vc{x}) + \dots \lambda_m h_m(\vc{x})$
        be the Lagrangian function. Let
        \begin{align*}
          \vc{F}(\vc{x}) =
          \begin{bmatrix}
            \frac{\partial^2 f}{\partial x_1^2}(\vc{x}) & \frac{\partial^2 f}{\partial x_2 \partial x_1}(\vc{x}) & \dots & \frac{\partial^2 f}{\partial x_n \partial x_1}(\vc{x}) \\
            \frac{\partial^2 f}{\partial x_1 \partial x_2}(\vc{x}) & \frac{\partial^2 f}{\partial x_2^2}(\vc{x}) & \dots & \frac{\partial^2 f}{\partial x_n \partial x_2}(\vc{x}) \\
            \vdots & \vdots & \ddots & \vdots \\
            \frac{\partial^2 f}{\partial x_1 \partial x_n}(\vc{x}) & \frac{\partial^2 f}{\partial x_2 \partial x_n}(\vc{x}) & \dots & \frac{\partial^2 f}{\partial x_n^2}(\vc{x})
          \end{bmatrix}
        \end{align*}
        be the Hessian matrix of $f$ at $\vc{x}$ and
        \begin{align*}
          \vc{H_k}(\vc{x}) =
          \begin{bmatrix}
            \frac{\partial^2 h_k}{\partial x_1^2}(\vc{x}) & \frac{\partial^2 h_k}{\partial x_2 \partial x_1}(\vc{x}) & \dots & \frac{\partial^2 h_k}{\partial x_n \partial x_1}(\vc{x}) \\
            \frac{\partial^2 h_k}{\partial x_1 \partial x_2}(\vc{x}) & \frac{\partial^2 h_k}{\partial x_2^2}(\vc{x}) & \dots & \frac{\partial^2 h_k}{\partial x_n \partial x_2}(\vc{x}) \\
            \vdots & \vdots & \ddots & \vdots \\
            \frac{\partial^2 h_k}{\partial x_1 \partial x_n}(\vc{x}) & \frac{\partial^2 h_k}{\partial x_2 \partial x_n}(\vc{x}) & \dots & \frac{\partial^2 h_k}{\partial x_n^2}(\vc{x})
          \end{bmatrix}
        \end{align*}
        be the Hessian matrix of $h_k$ at $\vc{x}$ for $k=1,\dots,m$. Define
        \begin{align*}
          \vc{L}(\vc{x},\vc{\lambda}) = \vc{F}(\vc{x}) + \lambda_1 \vc{H_1}(\vc{x}) + \dots + \lambda_m \vc{H_m}(\vc{x})
        \end{align*}
        to be the Hessian Matrix of $l(\vc{x}, \vc{\lambda})$ with respect to $\vc{x}$.

        Suppose there exists a point $\vc{x}^* \in \mathbb{R}^n$ and $\vc{\lambda^*} \in \mathbb{R}^m$ such that
        \begin{itemize}
          \item $D f(\vc{x}^*) + \vc{\lambda}^{*\tran} D \vc{h}(\vc{x}^*) = \vc{0}^\tran$.
          \item For all $\vc{y}\in T(\vc{x}^*)$, $\vc{y}\neq \vc{0}$, we have that $\vc{y}^\tran \vc{L}(\vc{x}^*, \vc{\lambda}^{*})\vc{y} < 0$, i.e.\ $\vc{L}(\vc{x}^*, \vc{\lambda}^{*})$ is negative definite on $T(\vc{x}^*)$.
        \end{itemize}
        Then $\vc{x}^*$ is a strict local maximizer of $f$ subject to $\vc{h}(\vc{x}) = \vc{0}.$
      \end{theorem}
  \end{enumerate}
\end{proof}
\newpage


% Problem 2
\begin{problem}
  Find local extremizers for the following optimization problem:
  \begin{align*}
    \begin{array}{rl}
      \text{maximize} & x_1x_2 \\
      \text{subject to} & x_1^2 + 4x_2^2 = 1.
    \end{array}
  \end{align*}
\end{problem}

\begin{proof}
  Lagrange's Theorem prescribes how to find the local extremizers for the optimization problem. Let $f(\vc{x}) = x_1x_2$
  and $h(\vc{x}) = x_1^2 + 4x_2^2 - 1$.
  Note that we then have that
  \begin{align*}
    \triangledown f(\vc{x})^\tran =
    \begin{bmatrix}
      x_2 &
      x_1
    \end{bmatrix}
    \quad \text{and} \quad
    \triangledown h(\vc{x})^\tran =
    \begin{bmatrix}
      2x_1 &
      8x_2
    \end{bmatrix}.
  \end{align*}
  Since for every feasible $\vc{x}$, the Jacobian is of rank 1, i.e.\ of full rank,
  every feasible point is a regular point. Using the Lagrange condition
  $D f(\vc{x}^*) + \vc{\lambda}^{*\tran} D \vc{h}(\vc{x}^*) = \vc{0}^\tran$, we formulate
  the system of equations
  \begin{align*}
    D f(\vc{x}^*) + \vc{\lambda}^{*\tran} D \vc{h}(\vc{x}^*) =
    \begin{bmatrix}x_2 + 2\lambda x_1 & x_1 + 8\lambda x_2\end{bmatrix} =
    \begin{bmatrix}0 & 0\end{bmatrix} = \vc{0}^\tran
  \end{align*}
  for $\lambda \in \mathbb{R}.$
  Thus, an extremizer of the optimization problem must satisfy the following system of equations
  \begin{align*}
    x_2 + 2\lambda x_1 &= 0 \\
    x_1 + 8 \lambda x_2 &= 0 \\
    x_1^2 + 4x_2^2  &=1.
  \end{align*}

  From the first two equations, we see that $x_2 = -2\lambda x_1$ and $x_1 = -8\lambda x_2$.
  These equations in conjunction show that either $x_1 = 0$, $x_2 = 0$, or $\lambda = \pm 1/4$. If $\vc{x}$ is a feasible point, then
  we can't have that $x_1 = 0$ nor $x_2=0$. Thus, we must have that $\lambda=\pm 1/4$. In that case, from the first equation,
  we have that $x_2 = \mp x_1/2$. Substituting this into the third equation yields that
  \begin{align*}
    x_1^2 + 4 (x_1^2/4) = 2x_1^2 = 1
  \end{align*}
  which implies that $x_1 = \pm 1 / \sqrt{2}$. Thus, $x_1 = \pm 1 /\sqrt{2}$ and $x_2 = \mp 1 / (2\sqrt{2})$ are the only solutions that satisfy the above system.
  That is, the extremizers to the optimization problem are
  \begin{align*}
    \vc{x}^{(1)} = \begin{bmatrix}1 / \sqrt{2} \\ 1 / (2\sqrt{2})\end{bmatrix},\
    \vc{x}^{(2)} = \begin{bmatrix}1 / \sqrt{2} \\ -1 / (2\sqrt{2})\end{bmatrix},\
    \vc{x}^{(3)} = \begin{bmatrix}-1 / \sqrt{2} \\ 1 / (2\sqrt{2})\end{bmatrix},\
    \vc{x}^{(4)} = \begin{bmatrix}-1 / \sqrt{2} \\ -1 / (2\sqrt{2})\end{bmatrix}.
  \end{align*}
  Since $f(\vc{x}^{(1)}) = f(\vc{x}^{(4)}) = 1/4$ and $f(\vc{x}^{(2)}) = f(\vc{x}^{(3)}) = -1/4$, we have that the possible
  maximizers of the problem are located at $\vc{x}^{(1)}$ and $\vc{x}^{(4)}$ while the
  possible minimizers of the problem are located at $\vc{x}^{(2)}$ and $\vc{x}^{(3)}$.
  Inspecting graphically, we can see that these are the maximizers and minimizers of the
  optimization problem.
\end{proof}
\newpage


% Problem 3
\begin{problem}
  Consider the problem
  \begin{align*}
    \begin{array}{rll}
      \text{minimize} & 2x_1 + 3x_2 - 4, & x_1, x_2 \in \mathbb{R} \\
      \text{subject to} & x_1 x_2 = 6.&
    \end{array}
  \end{align*}

  \begin{enumerate}
    \item Use Lagrange's theorem to find all possible local minimizers and maximizers.
    \item Use the second-order sufficient conditions to specify which points are strict
      local minimizers and which are strict local maximizers.
    \item Are the points in part b global minimizers or maximizers? Explain.
  \end{enumerate}
\end{problem}

\begin{proof}
\end{proof}
\newpage


% Problem 4
\begin{problem}
  Consider the problem of minimizing a general quadratic function subject to a linear constraint:
  \begin{align*}
    \begin{array}{rl}
      \text{minimize} & \frac{1}{2} \vc{x}^\tran Q \vc{x} - \vc{c}^\tran \vc{x} + d\\
      \text{subject to} & A\vc{x} = \vc{b},
    \end{array}
  \end{align*}
  where $Q=Q^\tran > 0$, $A \in\mathbb{R}^{m \times n}$ with $m<n$, $\text{rank} A = m$ and $d$ a constant.
  Derive a closed form solution to the problem.
\end{problem}

\begin{proof}
\end{proof}
\newpage


% Problem 5
\begin{problem}
  Consider the discrete-time linear system $x_k = 2 x_{k-1} + u_k$, $k \geq 1$, with
  $x_0 = 1$. Find the values of the control inputs $u_1$ and $u_2$ to minimize
  $$x_2^2 + \frac{1}{2}u_1^2+ \frac{1}{3}u_2^2.$$
\end{problem}

\begin{proof}
\end{proof}


\end{document}

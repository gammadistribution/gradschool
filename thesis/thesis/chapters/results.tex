\begin{chapter}{Results}

  Now that we have validated the model and identified its areas of discrepancies,
  we wish to evaluate the performance of the model in terms of its ability to
  generate forecasts. In order to make this evaluation, we will compare
  the performance of the model $\mathcal{M}$ to the baseline model $\mathcal{M}_0$
  used throughout the TV industry modified from the literature.

  Throughout this section, we perform the inference as described in the previous section
  and use the computed posterior density to generate the forecasts of the hold-out set.
  In order to remove any errors associated to inaccurate $m^B$ forecasts, we use the observed
  $m_i^B$ for each unit of observation from the hold-out set to generate the forecasts using model $\mathcal{M}$.
  We summarize the inferences produced by the probabilistic forecasts,
  as required in order to construct media plans, by taking their expected value when appropriate.

  \begin{section}{Industry Standard Model}
    Models used historically by the TV Industry rely on matching historical
    data to ``previously broadcast programs on the basis of program genre
    and various attributes \cite{tvforecasting}.'' The model typically then is some average of the observed historical data.

    To mirror this practice, we define the baseline industry model $\mathcal{M}_0$ as follows.
    Let $x_i$ be the covariate vector of the units of observation in the hold-out set
    containing the content identifier and the stratified hour and let $x_i'$ be the covariate vector
    containing only the stratified hour. Define the forecasted impression concentration
    of airing $i$ to be the average $c$ over the train set for each unit of observation that has the same covariate vector $x_i$.
    If no such average exists, i.\ e.\ the content has never aired before, define the
    forecasted impression concentration of airing $i$ to be the average $c$ over the train set for each unit of observation that has
    the same covariate vector $x_i'$. The forecasted $m_i^A$ for airing $i$ is then $c m_i^B $.
    This method ensures that we will generate a forecast for each unit of observation in the hold-out set
    and that we are generating the forecasts in a comparable way to model $\mathcal{M}$.
  \end{section}

  \begin{section}{Predictive Accuracy}
    We wish to understand the predictive accuracy of model $\mathcal{M}$ in comparison
    to the baseline model $\mathcal{M}_0$ to see if the forecasts generated by model $\mathcal{M}$ are more
    accurate than those generated by model $\mathcal{M}_0$.

    To evaluate this we will use for point estimates the measure of Mean Absolute Error (MAE)
    defined as $MAE = (1/ N)\sum_{i=1}^N|y_i^{\text{act}} - y_i^{\text{pred}}|$
    As we are generating probabilistic forecasts using model $\mathcal{M}$, it makes
    sense to evaluate their probabilities against the actual observed outcome.
    A generalization of the MAE exists for probabilistic forecasts known as the Continuous
    Ranked Probability Score (CRPS).

    Let $F$ be the cumulative distribution function of $X$, the forecasted quantity.
    If $x$ is the observed value, define the CRPS between $F$ and $x$ as
    \begin{align}\label{form:crps}
      \text{CRPS}(F, x) = \int_{-\infty}^{\infty}\left(F(y) - H(y - x)\right)^2dy
    \end{align}
    where $H$ is the Heaviside step function \cite{crps}.
    As we are using a simulated probabilistic forecast, we can compute \ref{form:crps}
    by discretizing appropriately using quadrature rules.

    To begin, we calculate the above error metrics of model $\mathcal{M}_0$ and $\mathcal{M}$
    at the level of the units of observation
    in comparison to the observed data of the hold-out set. The results of these calculations are stored
    in Table \ref{tab:uniterror}.

    \begin{table}[h!]
      \centering
        \begin{tabular}{llrrrr}
          network & level & $\overline{m_i^A}$ & $\mathcal{M}_0$ MAE & $\mathcal{M}$ CRPS & $\mathcal{M}$ MAE \\
          \hline
          BCST & unit of observation & 171450.06 & 23307.81 & 15247.91 & 21408.24 \\
          ETMT & unit of observation & 13034.77 & 5123.70 & 3683.04 & 5226.42 \\
          SPTS & unit of observation & 3164.84 & 1844.51 & 1426.60 & 1942.76
        \end{tabular}
      \caption{Error metrics of the predicted data and the observed data in the hold-out set. Note that
      $\mathcal{M}$ MAE is computed using the point forecast of the probabilistic forecasts. The average
      value of $m_i^A$ is presented to illustrate the magnitude of the errors.}\label{tab:uniterror}
    \end{table}

    From this table, we see that the errors of both models range in magnitude from 10-50\%
    of the observed mean. When using the point forecasts of model $\mathcal{M}$, the
    model performs similarly to the base model $\mathcal{M}_0$ where the base model slightly outperforms
    model $\mathcal{M}$ on the cable networks (ETMT and SPTS) and does slightly worse than model $\mathcal{M}$
    on the broadcast network BCST. However, we see that model $\mathcal{M}$ greatly outperforms the base model.
    Thus, in terms of this error metric, we conclude that model $\mathcal{M}$ has more predictive accuracy
    than the base model at the level of the units of observation.

    In addition to the error metrics listed above, we are interested in whether or not the forecasted
    probability distributions correctly assign probabilities to the possible outcomes. That is, we
    are interested in knowing if our probabilistic forecasts are calibrated. To assess this,
    for each unit of observation we can calculate a $(1 - \alpha)$ Credible Region (CR) for $\alpha = 0.5, 0.05, 0.01$
    and then measure whether the observed outcome was within that CR. If the forecasts are perfectly calibrated, then the proportion
    of events that are within the $(1 - \alpha)$ CR will be equal to $(1 - \alpha)$.
    From a Bayesian point of view, a $(1- \alpha)$ Credible Region is the region of the posterior density
    that contains all events that have a $(1- \alpha)$ chance of occurring.

    From table \ref{tab:unitcr}, we can see that the probabilistic forecasts for the units of observation
    are near perfectly calibrated for all networks except for SPTS. Upon closer inspection,
    we see that 89.66 \% of the observed outcomes that are less than the lower limit of the $(1 - \alpha)$ CR for $\alpha = 0.05$
    have an associated lower limit of less than 10. This distribution of lower limits is shown in
    Figure \ref{fig:cranalysis}.
    \begin{figure}[!h]
      \centering
      \includegraphics[scale=0.6]{SPTS_CR_analysis}
      \caption{Distribution of the left end point of the 95\% Credible Region for the SPTS network where the outcome was not within the Credible Region.}
      \label{fig:cranalysis}
    \end{figure}
    Thus, technically the observed outcomes are outside of the interval,
    but these lower limits are close enough to 0 for the practical purposes of TV planning.
    Removing the assumed right-censored data, i.\ e.\ data in which $m_i^A = 0$,
    shows that 91.8\% of units are within the 95\%, a result consistent with the other networks.
    This information is useful as we will be able to describe a range of possible outcomes for each
    unit of observation and be reasonably certain that the outcome will be within that range.

    \begin{table}[h!]
      \centering
        \begin{tabular}{lrrrr}
          & & \multicolumn{3}{c}{Credible Region $(1 - \alpha)$} \\
          network & level & $\alpha = 0.5$ & $\alpha = 0.05$ & $\alpha = 0.01$ \\
          \hline
          BCST & unit of observation & 0.465 & 0.922 & 0.966 \\
          ETMT & unit of observation & 0.48 & 0.91 & 0.95  \\
          SPTS & unit of observation & 0.416 & 0.765 & 0.799 \\
        \end{tabular}
      \caption{.}\label{tab:unitcr}
    \end{table}

  \end{section}

  \begin{section}{Aggregated Predictive Accuracy}

    Now that we have evaluated the accuracy of the forecasts on the units of observation,
    we want to
    The units are exchangeable under the model assumptions, and given the model parameters are iid,
    we can assume that the predictions under the model are independent. We can aggregate
    the forecasts through simple sums of the distribution of outcomes.


    \begin{table}[h!]
      \centering
        \begin{tabular}{lrrrr}
          network & level & $\mathcal{M}_0$ MAE & $\mathcal{M}$ CRPS & $\mathcal{M}$ MAE \\
          \hline
          BCST & quantiled & 324034.14 & 277437.29 & 417797.60 \\
          ETMT & quantiled & 297512.52 & 311316.38 & 399648.10 \\
          SPTS & quantiled & 205096.15 & 75962.55 & 90668.35
        \end{tabular}
      \caption{Example data set used for model for two selling title airings.}\label{tab:quantileerror}
    \end{table}

    \begin{table}[h!]
      \centering
        \begin{tabular}{lrrrr}
          & & \multicolumn{3}{c}{Credible Region $(1 - \alpha)$} \\
          network & level & $\alpha = 0.5$ & $\alpha = 0.05$ & $\alpha = 0.01$ \\
          \hline
          BCST & quantiled & 0.45 & 0.95 & 1.0 \\
          ETMT & quantiled & 0.1 & 0.55  & 0.75 \\
          SPTS & quantiled & 0.4 & 0.65 & 0.65 \\
        \end{tabular}
      \caption{Example data set used for model for two selling title airings.}\label{tab:quantilecr}
    \end{table}

    \begin{figure}[!h]
      \centering
      \begin{subfigure}[b]{.75\textwidth}
        \centering
        \includegraphics[scale=0.6]{BCST_quantiled}
      \end{subfigure}
      \begin{subfigure}[b]{.75\textwidth}
        \centering
        \includegraphics[scale=0.6]{ETMT_quantiled}
      \end{subfigure}
      \begin{subfigure}[b]{.75\textwidth}
        \centering
        \includegraphics[scale=0.6]{SPTS_quantiled}
      \end{subfigure}
      \caption{Test.}
      \label{fig:quantiledoutcomes}
    \end{figure}

    \begin{subsection}{Random Media Plans}
      Create media plans by randomly selecting airings and then compute HPD and mean
      of media plan where we assume that the demo estimates are known.

      We want to check that outcomes are within are 95\% HPD
    \end{subsection}

    \begin{subsection}{Calibration}
      Show calibration plot of binning up all airings into 20 or so bins
      and then plotting mean of actual vs mean of predicted. If model is calibrated
      then ``low stuff should perform low and the high stuff high.''
    \end{subsection}
  \end{section}
\end{chapter}
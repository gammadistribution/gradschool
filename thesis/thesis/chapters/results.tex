\begin{chapter}{Forecast Results}

  We want to compare the forecasts from the posterior predictive distribution
  on test data vs the industry standard forecasts per Tv ratings paper.

  \begin{section}{Predictive Accuracy}
    Compute probabilistic scores for bayes forecast and compute MAE for industry
    standard. Compare vs baseline of population proportion.

    Show actual vs predicted plots of each airing

    Walk through example that illustrates the added benefit of model
  \end{section}

  \begin{section}{Media plans}

    \begin{subsection}{Random Media Plans}
      Create media plans by randomly selecting airings and then compute HPD and mean
      of media plan where we assume that the demo estimates are known.

      We want to check that outcomes are within are 95\% HPD
    \end{subsection}

    \begin{subsection}{Calibration}
      Show calibration plot of binning up all airings into 20 or so bins
      and then plotting mean of actual vs mean of predicted. If model is calibrated
      then ``low stuff should perform low and the high stuff high.''
    \end{subsection}
  \end{section}

  \begin{section}{New Content}
    We expect the model to perform better on new content than industry standard
    method. We should illustrate that here.
  \end{section}

  \begin{section}{Inference Summary}
    We summarize the inferences using the mean of the posterior.
  \end{section}

\end{chapter}